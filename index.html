<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>T2I Benchmark Review: 2022-2025</title>
    <link rel="icon" type="image/png" href="images/favicon.png">
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">
    <style>
        body { font-family: 'Inter', sans-serif; background-color: #fdfaf6; color: #333; }
        .chart-container {
            position: relative;
            width: 100%;
            max-width: 600px;
            height: 350px;
            max-height: 400px;
            margin: 0 auto;
        }
        .card-hover:hover { transform: translateY(-4px); box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05); cursor: pointer; }
        .transition-all { transition-property: all; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); transition-duration: 300ms; }
        .active-filter { background-color: #d97706; color: white; border-color: #d97706; }
        
        /* Modal Styles */
        .modal-overlay {
            background-color: rgba(0, 0, 0, 0.5);
            backdrop-filter: blur(4px);
        }
        .prose-sm ul { list-style-type: disc; padding-left: 1.5em; }

        /* Custom Scrollbar for the long future section if needed */
        .custom-scroll::-webkit-scrollbar { width: 6px; }
        .custom-scroll::-webkit-scrollbar-track { background: #f1f1f1; }
        .custom-scroll::-webkit-scrollbar-thumb { background: #d97706; border-radius: 4px; }
    </style>
    <!-- Chosen Palette: Warm Neutrals (Background: #fdfaf6) with Data Viz Colors (Amber, Teal, Indigo, Rose) -->
    <!-- Application Structure Plan: 
         1. Hero: Context setting.
         2. Interactive Benchmark Database: Grid of cards. Updated with TIFF, FineGrain, and T2I-CompBench++.
         3. Analytics Dashboard: Visualizing shift in evaluation methods.
         4. Future Trends: "Now vs Next" comparison with detailed strategic pillars.
    -->
    <!-- CONFIRMATION: NO SVG graphics used. NO Mermaid JS used. -->
</head>
<body class="bg-[#fdfaf6] min-h-screen flex flex-col">

    <!-- Navigation -->
    <nav class="bg-white border-b border-gray-200 sticky top-0 z-40">
        <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
            <div class="flex justify-between h-16">
                <div class="flex items-center">
                    <span class="text-2xl font-bold text-gray-800 tracking-tight">T2I <span class="text-amber-600">Eval</span>Lens</span>
                </div>
                <div class="flex items-center space-x-8">
                    <button onclick="scrollToSection('database')" class="text-gray-600 hover:text-amber-600 font-medium hidden sm:block">Benchmarks</button>
                    <button onclick="scrollToSection('analytics')" class="text-gray-600 hover:text-amber-600 font-medium hidden sm:block">Trends Data</button>
                    <button onclick="scrollToSection('future')" class="text-gray-600 hover:text-amber-600 font-medium">The Future</button>
                </div>
            </div>
        </div>
    </nav>

    <!-- Header -->
    <header class="bg-white pb-12 pt-10">
        <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
            <h1 class="text-4xl md:text-5xl font-bold text-gray-900 mb-4">
                The State of Text-to-Image Evaluation
                <span class="block text-amber-600 text-3xl md:text-4xl mt-2">2022 &mdash; 2025</span>
            </h1>
            <p class="text-xl text-gray-600 max-w-3xl leading-relaxed">
                A comprehensive review of the benchmarks defining the T2I landscape. 
                Click on any card to explore the methodology, detailed findings, and access the original papers.
            </p>
        </div>
    </header>

    <!-- Main Content -->
    <main class="flex-grow">

        <!-- Section 1: Benchmark Database -->
        <section id="database" class="py-12 max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
            <div class="mb-8">
                <div class="flex flex-col md:flex-row md:items-end justify-between mb-6">
                    <div>
                        <h2 class="text-3xl font-bold text-gray-900 mb-2">Benchmark Database</h2>
                        <p class="text-gray-600">
                            Explore 3 years of evaluation frameworks. 
                            <span class="font-semibold text-amber-600">Click a card for details & links.</span>
                        </p>
                    </div>
                </div>
                
                <!-- Metric Filters -->
                <div class="flex flex-wrap gap-2 items-center mb-6">
                    <span class="text-xs font-bold text-gray-400 uppercase tracking-wider mr-2">Metrics:</span>
                    <button class="filter-btn px-4 py-2 rounded-full text-sm font-medium transition-all duration-200 bg-white text-gray-600 border border-gray-200 hover:bg-gray-50" data-filter="metric" data-value="Human">Human</button>
                    <button class="filter-btn px-4 py-2 rounded-full text-sm font-medium transition-all duration-200 bg-white text-gray-600 border border-gray-200 hover:bg-gray-50" data-filter="metric" data-value="CLIP">CLIP</button>
                    <button class="filter-btn px-4 py-2 rounded-full text-sm font-medium transition-all duration-200 bg-white text-gray-600 border border-gray-200 hover:bg-gray-50" data-filter="metric" data-value="VQA">VQA</button>
                    <button class="filter-btn px-4 py-2 rounded-full text-sm font-medium transition-all duration-200 bg-white text-gray-600 border border-gray-200 hover:bg-gray-50" data-filter="metric" data-value="BLIP">BLIP</button>
                    <button class="filter-btn px-4 py-2 rounded-full text-sm font-medium transition-all duration-200 bg-white text-gray-600 border border-gray-200 hover:bg-gray-50" data-filter="metric" data-value="Object Detection">Obj Det</button>
                </div>
                
                <!-- Filter Section -->
        <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8">
            
            <!-- Search and Primary Filters -->
            <div class="flex flex-col md:flex-row gap-6 items-center justify-between mb-8">
                <!-- Search -->
                <div class="relative w-full md:w-96">
                    <div class="absolute inset-y-0 left-0 pl-3 flex items-center pointer-events-none">
                        <svg class="h-5 w-5 text-gray-400" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"></path></svg>
                    </div>
                    <input type="text" id="search-input" class="block w-full pl-10 pr-3 py-2 border border-gray-300 rounded-full leading-5 bg-white placeholder-gray-500 focus:outline-none focus:placeholder-gray-400 focus:ring-1 focus:ring-amber-500 focus:border-amber-500 sm:text-sm transition-shadow" placeholder="Search benchmarks, targets, descriptions..." oninput="renderGrid()">
                </div>

                <!-- Year Filters -->
                <div class="flex flex-wrap gap-2 justify-center">
                    <span class="text-xs font-bold text-gray-400 uppercase tracking-wider mr-2 self-center">Year:</span>
                    <button class="filter-btn active-filter px-4 py-2 rounded-full text-sm font-medium transition-all duration-200 bg-amber-600 text-white shadow-md transform scale-105" data-filter="year" data-value="all">All</button>
                    <button class="filter-btn px-4 py-2 rounded-full text-sm font-medium transition-all duration-200 bg-white text-gray-600 border border-gray-200 hover:bg-gray-50" data-filter="year" data-value="2025">2025</button>
                    <button class="filter-btn px-4 py-2 rounded-full text-sm font-medium transition-all duration-200 bg-white text-gray-600 border border-gray-200 hover:bg-gray-50" data-filter="year" data-value="2024">2024</button>
                    <button class="filter-btn px-4 py-2 rounded-full text-sm font-medium transition-all duration-200 bg-white text-gray-600 border border-gray-200 hover:bg-gray-50" data-filter="year" data-value="2023">2023</button>
                    <button class="filter-btn px-4 py-2 rounded-full text-sm font-medium transition-all duration-200 bg-white text-gray-600 border border-gray-200 hover:bg-gray-50" data-filter="year" data-value="2022">2022</button>
                </div>
            </div>

            <!-- Category Filters -->
            <div class="flex flex-wrap gap-2 items-center justify-center mb-8">
                <span class="text-xs font-bold text-gray-400 uppercase tracking-wider mr-2">Focus:</span>
                <button class="filter-btn active-filter px-4 py-2 rounded-full text-sm font-medium transition-all duration-200 bg-amber-600 text-white shadow-md transform scale-105" data-filter="category" data-value="all">All Focus</button>
                <button class="filter-btn px-4 py-2 rounded-full text-sm font-medium transition-all duration-200 bg-white text-gray-600 border border-gray-200 hover:bg-gray-50" data-filter="category" data-value="composition">Composition</button>
                <button class="filter-btn px-4 py-2 rounded-full text-sm font-medium transition-all duration-200 bg-white text-gray-600 border border-gray-200 hover:bg-gray-50" data-filter="category" data-value="reasoning">Reasoning</button>
                <button class="filter-btn px-4 py-2 rounded-full text-sm font-medium transition-all duration-200 bg-white text-gray-600 border border-gray-200 hover:bg-gray-50" data-filter="category" data-value="diagram">Diagram</button>
                <button class="filter-btn px-4 py-2 rounded-full text-sm font-medium transition-all duration-200 bg-white text-gray-600 border border-gray-200 hover:bg-gray-50" data-filter="category" data-value="both">Both</button>
            </div>

            <!-- Result Count -->
            <div class="text-center mb-6">
                <span id="result-count" class="text-sm font-medium text-gray-500">Showing all results</span>
            </div>

            <!-- Grid -->
            <div id="benchmark-grid" class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-8">
                <!-- Cards injected by JS -->
            </div>
        </div>
        </section>

        <!-- Section 1.5: Comparison Table -->
        <section id="comparison" class="py-12 bg-gray-50 border-t border-gray-200">
            <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
                <div class="mb-8">
                    <h2 class="text-3xl font-bold text-gray-900 mb-4">Benchmark Comparison</h2>
                    <p class="text-gray-600 max-w-3xl">
                        Comparison between T2I-CoReBench and existing T2I benchmarks. T2I-CoReBench comprehensively covers 12 evaluation dimensions spanning both composition and reasoning scenarios.
                    </p>
                    <div class="mt-4 flex items-center gap-6 text-sm text-gray-600">
                        <span class="flex items-center gap-2"><span class="text-lg">●</span> High-complexity coverage</span>
                        <span class="flex items-center gap-2"><span class="text-lg">◐</span> Simple coverage</span>
                        <span class="flex items-center gap-2"><span class="text-lg">○</span> Not covered</span>
                    </div>
                </div>

                <div class="overflow-x-auto bg-white rounded-xl shadow-sm border border-gray-200">
                    <table class="w-full text-sm text-center">
                        <thead>
                            <tr class="bg-gray-100 text-gray-700 font-semibold border-b border-gray-200">
                                <th rowspan="3" class="px-4 py-3 text-left bg-gray-50 sticky left-0 z-10 border-r border-gray-200">Benchmarks</th>
                                <th colspan="4" class="px-4 py-2 border-r border-gray-200 bg-amber-50 text-amber-900">Composition</th>
                                <th colspan="8" class="px-4 py-2 bg-indigo-50 text-indigo-900">Reasoning</th>
                            </tr>
                            <tr class="bg-gray-50 text-gray-600 text-xs uppercase tracking-wide border-b border-gray-200">
                                <!-- Composition -->
                                <th colspan="4" class="border-r border-gray-200 bg-amber-50/50"></th>
                                <!-- Reasoning -->
                                <th colspan="4" class="border-r border-gray-200 bg-indigo-50/50">Deductive</th>
                                <th colspan="2" class="border-r border-gray-200 bg-indigo-50/50">Inductive</th>
                                <th colspan="2" class="bg-indigo-50/50">Abductive</th>
                            </tr>
                            <tr class="text-xs text-gray-500 border-b border-gray-200">
                                <!-- Composition Headers -->
                                <th class="px-2 py-2 w-12 cursor-help" title="Multi-Instance">MI</th>
                                <th class="px-2 py-2 w-12 cursor-help" title="Multi-Attribute">MA</th>
                                <th class="px-2 py-2 w-12 cursor-help" title="Multi-Relation">MR</th>
                                <th class="px-2 py-2 w-12 border-r border-gray-200 cursor-help" title="Text Rendering">TR</th>
                                <!-- Reasoning Headers -->
                                <th class="px-2 py-2 w-12 cursor-help" title="Logical Reasoning">LR</th>
                                <th class="px-2 py-2 w-12 cursor-help" title="Behavioral Reasoning">BR</th>
                                <th class="px-2 py-2 w-12 cursor-help" title="Hypothetical Reasoning">HR</th>
                                <th class="px-2 py-2 w-12 border-r border-gray-200 cursor-help" title="Procedural Reasoning">PR</th>
                                <th class="px-2 py-2 w-12 cursor-help" title="Generalization Reasoning">GR</th>
                                <th class="px-2 py-2 w-12 border-r border-gray-200 cursor-help" title="Analogical Reasoning">AR</th>
                                <th class="px-2 py-2 w-12 cursor-help" title="Commonsense Reasoning">CR</th>
                                <th class="px-2 py-2 w-12 cursor-help" title="Reconstructive Reasoning">RR</th>
                            </tr>
                        </thead>
                        <tbody class="divide-y divide-gray-100">
                            <!-- Rows -->
                            <tr class="hover:bg-gray-50 transition-colors">
                                <td class="px-4 py-3 text-left font-medium text-blue-600 sticky left-0 bg-white border-r border-gray-100"><a href="https://arxiv.org/abs/2307.06350" target="_blank" class="hover:underline hover:text-blue-800">T2I-CompBench</a></td>
                                <td>◐</td><td>◐</td><td>◐</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td>○</td><td>○</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td>○</td>
                            </tr>
                            <tr class="hover:bg-gray-50 transition-colors">
                                <td class="px-4 py-3 text-left font-medium text-blue-600 sticky left-0 bg-white border-r border-gray-100"><a href="https://arxiv.org/abs/2310.11513" target="_blank" class="hover:underline hover:text-blue-800">GenEval</a></td>
                                <td>◐</td><td>◐</td><td>◐</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td>○</td><td>○</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td>○</td>
                            </tr>
                            <tr class="hover:bg-gray-50 transition-colors">
                                <td class="px-4 py-3 text-left font-medium text-blue-600 sticky left-0 bg-white border-r border-gray-100"><a href="https://arxiv.org/abs/2406.13743" target="_blank" class="hover:underline hover:text-blue-800">GenAI-Bench</a></td>
                                <td>◐</td><td>◐</td><td>◐</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td>○</td><td>○</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td>○</td>
                            </tr>
                            <tr class="hover:bg-gray-50 transition-colors">
                                <td class="px-4 py-3 text-left font-medium text-blue-600 sticky left-0 bg-white border-r border-gray-100"><a href="https://arxiv.org/abs/2403.05135" target="_blank" class="hover:underline hover:text-blue-800">DPG-Bench</a></td>
                                <td>●</td><td>●</td><td>●</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td>○</td><td>○</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td>○</td>
                            </tr>
                            <tr class="hover:bg-gray-50 transition-colors">
                                <td class="px-4 py-3 text-left font-medium text-blue-600 sticky left-0 bg-white border-r border-gray-100"><a href="https://arxiv.org/abs/2408.14339" target="_blank" class="hover:underline hover:text-blue-800">ConceptMix</a></td>
                                <td>◐</td><td>◐</td><td>◐</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td>○</td><td>○</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td>○</td>
                            </tr>
                            <tr class="hover:bg-gray-50 transition-colors">
                                <td class="px-4 py-3 text-left font-medium text-blue-600 sticky left-0 bg-white border-r border-gray-100"><a href="https://arxiv.org/abs/2506.02161" target="_blank" class="hover:underline hover:text-blue-800">TIIF-Bench</a></td>
                                <td>◐</td><td>◐</td><td>◐</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td>○</td><td>○</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td>○</td>
                            </tr>
                            <tr class="hover:bg-gray-50 transition-colors">
                                <td class="px-4 py-3 text-left font-medium text-blue-600 sticky left-0 bg-white border-r border-gray-100"><a href="https://arxiv.org/abs/2505.24787" target="_blank" class="hover:underline hover:text-blue-800">LongBench-T2I</a></td>
                                <td>●</td><td>●</td><td>●</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td>○</td><td>○</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td>○</td>
                            </tr>
                            <tr class="hover:bg-gray-50 transition-colors">
                                <td class="px-4 py-3 text-left font-medium text-blue-600 sticky left-0 bg-white border-r border-gray-100"><a href="https://arxiv.org/abs/2509.09680" target="_blank" class="hover:underline hover:text-blue-800">PRISM-Bench</a></td>
                                <td>●</td><td>◐</td><td>●</td><td class="border-r border-gray-100">◐</td>
                                <td>○</td><td>○</td><td>○</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td>○</td>
                            </tr>
                            <tr class="hover:bg-gray-50 transition-colors">
                                <td class="px-4 py-3 text-left font-medium text-blue-600 sticky left-0 bg-white border-r border-gray-100"><a href="https://arxiv.org/abs/2510.18701" target="_blank" class="hover:underline hover:text-blue-800">UniGenBench</a></td>
                                <td>◐</td><td>◐</td><td>◐</td><td class="border-r border-gray-100">◐</td>
                                <td>◐</td><td>○</td><td>○</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td class="border-r border-gray-100">○</td>
                                <td>◐</td><td>○</td>
                            </tr>
                            <tr class="hover:bg-gray-50 transition-colors">
                                <td class="px-4 py-3 text-left font-medium text-blue-600 sticky left-0 bg-white border-r border-gray-100"><a href="https://arxiv.org/abs/2406.07546" target="_blank" class="hover:underline hover:text-blue-800">Commonsense-T2I</a></td>
                                <td>○</td><td>○</td><td>○</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td>○</td><td>○</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td class="border-r border-gray-100">○</td>
                                <td>◐</td><td>○</td>
                            </tr>
                            <tr class="hover:bg-gray-50 transition-colors">
                                <td class="px-4 py-3 text-left font-medium text-blue-600 sticky left-0 bg-white border-r border-gray-100"><a href="https://arxiv.org/abs/2406.11802" target="_blank" class="hover:underline hover:text-blue-800">PhyBench</a></td>
                                <td>○</td><td>○</td><td>○</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td>◐</td><td>○</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td class="border-r border-gray-100">○</td>
                                <td>◐</td><td>○</td>
                            </tr>
                            <tr class="hover:bg-gray-50 transition-colors">
                                <td class="px-4 py-3 text-left font-medium text-blue-600 sticky left-0 bg-white border-r border-gray-100"><a href="https://arxiv.org/abs/2503.07265" target="_blank" class="hover:underline hover:text-blue-800">WISE</a></td>
                                <td>○</td><td>○</td><td>○</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td>○</td><td>○</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td class="border-r border-gray-100">○</td>
                                <td>◐</td><td>○</td>
                            </tr>
                            <tr class="hover:bg-gray-50 transition-colors">
                                <td class="px-4 py-3 text-left font-medium text-blue-600 sticky left-0 bg-white border-r border-gray-100"><a href="https://arxiv.org/abs/2508.17472" target="_blank" class="hover:underline hover:text-blue-800">T2I-ReasonBench</a></td>
                                <td>○</td><td>○</td><td>○</td><td class="border-r border-gray-100">◐</td>
                                <td>○</td><td>○</td><td>○</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td class="border-r border-gray-100">○</td>
                                <td>◐</td><td>○</td>
                            </tr>
                            <tr class="hover:bg-gray-50 transition-colors">
                                <td class="px-4 py-3 text-left font-medium text-blue-600 sticky left-0 bg-white border-r border-gray-100"><a href="https://arxiv.org/abs/2505.23493" target="_blank" class="hover:underline hover:text-blue-800">R2I-Bench</a></td>
                                <td>○</td><td>○</td><td>◐</td><td class="border-r border-gray-100">○</td>
                                <td>◐</td><td>◐</td><td>◐</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td class="border-r border-gray-100">○</td>
                                <td>◐</td><td>◐</td>
                            </tr>
                            <tr class="hover:bg-gray-50 transition-colors">
                                <td class="px-4 py-3 text-left font-medium text-blue-600 sticky left-0 bg-white border-r border-gray-100"><a href="https://arxiv.org/abs/2506.07977" target="_blank" class="hover:underline hover:text-blue-800">OneIG-Bench</a></td>
                                <td>●</td><td>●</td><td>●</td><td class="border-r border-gray-100">●</td>
                                <td>○</td><td>○</td><td>○</td><td class="border-r border-gray-100">○</td>
                                <td>○</td><td class="border-r border-gray-100">○</td>
                                <td>◐</td><td>○</td>
                            </tr>
                            <tr class="bg-gray-900 text-white font-bold hover:bg-gray-800 transition-colors">
                                <td class="px-4 py-3 text-left sticky left-0 bg-gray-900 border-r border-gray-700"><a href="https://arxiv.org/abs/2509.03516" target="_blank" class="hover:underline hover:text-blue-300">T2I-CoReBench</a></td>
                                <td>●</td><td>●</td><td>●</td><td class="border-r border-gray-700">●</td>
                                <td>●</td><td>●</td><td>●</td><td class="border-r border-gray-700">●</td>
                                <td>●</td><td class="border-r border-gray-700">●</td>
                                <td>●</td><td>●</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <!-- Table Legend / Caption -->
                <div class="mt-6 text-sm text-gray-600 space-y-3 bg-white p-4 rounded-lg border border-gray-200">
                    <p class="font-medium text-gray-900">Table 1: T2I benchmark comparison.</p>
                    <p>
                        Our <strong>T2I-CoReBench</strong> comprehensively covers 12 evaluation dimensions spanning both composition 
                        (<span class="font-semibold" title="Multi-Instance">MI</span> Multi-Instance, <span class="font-semibold" title="Multi-Attribute">MA</span> Multi-Attribute, <span class="font-semibold" title="Multi-Relation">MR</span> MultiRelation, <span class="font-semibold" title="Text Rendering">TR</span> Text Rendering) 
                        and reasoning 
                        (<span class="font-semibold" title="Logical Reasoning">LR</span> Logical Reasoning, <span class="font-semibold" title="Behavioral Reasoning">BR</span> Behavioral Reasoning, <span class="font-semibold" title="Hypothetical Reasoning">HR</span> Hypothetical Reasoning, <span class="font-semibold" title="Procedural Reasoning">PR</span> Procedural Reasoning, <span class="font-semibold" title="Generalization Reasoning">GR</span> Generalization Reasoning, <span class="font-semibold" title="Analogical Reasoning">AR</span> Analogical Reasoning, <span class="font-semibold" title="Commonsense Reasoning">CR</span> Commonsense Reasoning, and <span class="font-semibold" title="Reconstructive Reasoning">RR</span> Reconstructive Reasoning).
                    </p>
                    <p>
                        The symbols denote different coverage levels: 
                        <span class="inline-flex items-center gap-1 font-medium text-gray-900"><span class="text-lg">●</span> indicates high compositional (visual elements > 5) or reasoning (one-to-many or many-to-one inference) complexity,</span>
                        <span class="inline-flex items-center gap-1 font-medium text-gray-900"><span class="text-lg">◐</span> indicates simple settings (visual elements ≤ 5 or one-to-one inference),</span>
                        and <span class="inline-flex items-center gap-1 font-medium text-gray-900"><span class="text-lg">○</span> indicates no coverage.</span>
                    </p>
                </div>


            <!-- Diagram Comparison Table -->
            <div class="mb-16 mt-16">
                <h3 class="text-xl font-bold text-gray-900 mb-6 flex items-center gap-2">
                    <svg class="w-6 h-6 text-amber-600" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 17v-2m3 2v-4m3 4v-6m2 10H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path>
                    </svg>
                    Diagram Benchmark Comparison
                </h3>
                <div class="overflow-hidden rounded-xl border border-gray-200 shadow-lg ring-1 ring-black ring-opacity-5">
                    <table class="w-full text-sm text-gray-600">
                        <thead class="bg-gray-50 text-xs uppercase font-bold text-gray-500 tracking-wider">
                            <tr>
                                <th class="px-6 py-5 text-left bg-gray-100 border-b border-gray-200">Feature</th>
                                <th class="px-6 py-5 text-center text-blue-800 bg-blue-50 border-b-2 border-blue-500 relative">
                                    <span class="absolute top-0 left-0 w-full h-1 bg-blue-500"></span>
                                    DiagramT2I (Ours)
                                    <span class="block text-[10px] font-normal text-blue-600 normal-case mt-1">Proposed Benchmark</span>
                                </th>
                                <th class="px-6 py-5 text-center border-b border-gray-200">DiagrammerGPT</th>
                                <th class="px-6 py-5 text-center border-b border-gray-200">DiagramGenBenchmark</th>
                            </tr>
                        </thead>
                        <tbody class="divide-y divide-gray-100 bg-white">
                            <!-- Main Focus -->
                            <tr class="group hover:bg-gray-50 transition-colors">
                                <td class="px-6 py-5 font-bold text-gray-900 bg-gray-50/50 group-hover:bg-gray-100/50 transition-colors">Main Focus</td>
                                <td class="px-6 py-5 text-center font-medium text-blue-900 bg-blue-50/30 group-hover:bg-blue-50/50 transition-colors border-x border-blue-100">
                                    Pure T2I Models
                                </td>
                                <td class="px-6 py-5 text-center">
                                    <div class="flex flex-col items-center gap-1">
                                        <span class="font-medium text-gray-900">Two-stage Pipeline</span>
                                        <span class="text-xs text-gray-500 bg-gray-100 px-2 py-0.5 rounded-full">LLM Planning + Rendering</span>
                                    </div>
                                </td>
                                <td class="px-6 py-5 text-center">
                                    <div class="flex flex-col items-center gap-1">
                                        <span class="font-medium text-gray-900">Agentic Pipeline</span>
                                        <span class="text-xs text-gray-500 bg-gray-100 px-2 py-0.5 rounded-full">Multi-agent System</span>
                                    </div>
                                </td>
                            </tr>
                            <!-- Metrics -->
                            <tr class="group hover:bg-gray-50 transition-colors">
                                <td class="px-6 py-5 font-bold text-gray-900 bg-gray-50/50 group-hover:bg-gray-100/50 transition-colors">Metrics</td>
                                <td class="px-6 py-5 text-center bg-blue-50/30 group-hover:bg-blue-50/50 transition-colors border-x border-blue-100">
                                    <div class="flex flex-col items-center gap-1">
                                        <span class="font-bold text-blue-700">Upgraded VPEval</span>
                                        <span class="text-xs text-blue-600 bg-blue-100 px-2 py-1 rounded-md">Enhanced labeling & part ID</span>
                                    </div>
                                </td>
                                <td class="px-6 py-5 text-center">
                                    <span class="font-medium text-gray-900">VPEval</span>
                                </td>
                                <td class="px-6 py-5 text-center">
                                    <div class="flex flex-col items-center gap-1">
                                        <span class="font-medium text-gray-900">Pass@1, Similarity</span>
                                        <span class="text-xs text-gray-500">Code-based evaluation</span>
                                    </div>
                                </td>
                            </tr>
                            <!-- Categories -->
                            <tr class="group hover:bg-gray-50 transition-colors">
                                <td class="px-6 py-5 font-bold text-gray-900 bg-gray-50/50 group-hover:bg-gray-100/50 transition-colors">Categories</td>
                                <td class="px-6 py-5 text-center bg-blue-50/30 group-hover:bg-blue-50/50 transition-colors border-x border-blue-100">
                                    <div class="flex flex-col items-center gap-2">
                                        <span class="text-2xl font-bold text-blue-700">11</span>
                                        <span class="text-xs font-medium text-blue-600 uppercase tracking-wide">Categories</span>
                                        <span class="text-xs text-blue-500 bg-white border border-blue-200 px-2 py-1 rounded-full shadow-sm mb-2">8 + 3 Extra</span>
                                        <div class="flex flex-wrap justify-center gap-1 max-w-[240px]">
                                            <span class="px-1.5 py-0.5 text-[10px] bg-blue-100 text-blue-700 rounded border border-blue-200">Model Arch</span>
                                            <span class="px-1.5 py-0.5 text-[10px] bg-blue-100 text-blue-700 rounded border border-blue-200">Flow Chart</span>
                                            <span class="px-1.5 py-0.5 text-[10px] bg-blue-100 text-blue-700 rounded border border-blue-200">Directed Graph</span>
                                            <span class="px-1.5 py-0.5 text-[10px] bg-blue-100 text-blue-700 rounded border border-blue-200">Undirected Graph</span>
                                            <span class="px-1.5 py-0.5 text-[10px] bg-blue-100 text-blue-700 rounded border border-blue-200">Line Chart</span>
                                            <span class="px-1.5 py-0.5 text-[10px] bg-blue-100 text-blue-700 rounded border border-blue-200">Bar Chart</span>
                                            <span class="px-1.5 py-0.5 text-[10px] bg-blue-100 text-blue-700 rounded border border-blue-200">Mindmap</span>
                                            <span class="px-1.5 py-0.5 text-[10px] bg-blue-100 text-blue-700 rounded border border-blue-200">Table</span>
                                            <!-- Extra 3 -->
                                            <span class="px-1.5 py-0.5 text-[10px] bg-amber-100 text-amber-700 rounded border border-amber-200 font-medium">Timeline</span>
                                            <span class="px-1.5 py-0.5 text-[10px] bg-amber-100 text-amber-700 rounded border border-amber-200 font-medium">Object Parts</span>
                                            <span class="px-1.5 py-0.5 text-[10px] bg-amber-100 text-amber-700 rounded border border-amber-200 font-medium">Pie Chart</span>
                                        </div>
                                    </div>
                                </td>
                                <td class="px-6 py-5 text-center">
                                    <div class="flex flex-wrap justify-center gap-1 max-w-[200px] mx-auto">
                                        <span class="px-2 py-1 text-xs bg-gray-100 text-gray-600 rounded-md border border-gray-200">Astrology</span>
                                        <span class="px-2 py-1 text-xs bg-gray-100 text-gray-600 rounded-md border border-gray-200">Engineering</span>
                                        <span class="px-2 py-1 text-xs bg-gray-100 text-gray-600 rounded-md border border-gray-200">Biology</span>
                                        <span class="px-2 py-1 text-xs bg-gray-100 text-gray-600 rounded-md border border-gray-200">Geology</span>
                                        <span class="px-2 py-1 text-xs bg-gray-100 text-gray-600 rounded-md border border-gray-200">Plants</span>
                                    </div>
                                </td>
                                <td class="px-6 py-5 text-center">
                                    <div class="flex flex-col items-center gap-2">
                                        <span class="text-xl font-bold text-gray-700">8</span>
                                        <span class="text-xs font-medium text-gray-500 uppercase tracking-wide">Categories</span>
                                        <div class="flex flex-wrap justify-center gap-1 max-w-[200px] mt-1">
                                            <span class="px-1.5 py-0.5 text-[10px] bg-gray-100 text-gray-600 rounded border border-gray-200">Model Arch</span>
                                            <span class="px-1.5 py-0.5 text-[10px] bg-gray-100 text-gray-600 rounded border border-gray-200">Flow Chart</span>
                                            <span class="px-1.5 py-0.5 text-[10px] bg-gray-100 text-gray-600 rounded border border-gray-200">Directed Graph</span>
                                            <span class="px-1.5 py-0.5 text-[10px] bg-gray-100 text-gray-600 rounded border border-gray-200">Undirected Graph</span>
                                            <span class="px-1.5 py-0.5 text-[10px] bg-gray-100 text-gray-600 rounded border border-gray-200">Line Chart</span>
                                            <span class="px-1.5 py-0.5 text-[10px] bg-gray-100 text-gray-600 rounded border border-gray-200">Bar Chart</span>
                                            <span class="px-1.5 py-0.5 text-[10px] bg-gray-100 text-gray-600 rounded border border-gray-200">Mindmap</span>
                                            <span class="px-1.5 py-0.5 text-[10px] bg-gray-100 text-gray-600 rounded border border-gray-200">Table</span>
                                        </div>
                                    </div>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
            </div>
        </section>

        <!-- Section 2: Analytics -->
        <section id="analytics" class="py-12 bg-white border-t border-gray-200">
            <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
                <h2 class="text-3xl font-bold text-gray-900 mb-4">Meta-Analysis</h2>
                <p class="text-gray-600 mb-10 max-w-3xl">
                    Analyzing the trajectory of evaluation techniques. Notice the sharp rise in automated pipelines using VQA (Visual Question Answering) and MLLMs to replace costly human annotation.
                </p>

                <div class="grid grid-cols-1 lg:grid-cols-2 gap-12">
                    <!-- Chart 1: Evaluation Type Trend -->
                    <div class="flex flex-col">
                        <h3 class="text-xl font-semibold text-gray-800 mb-4 text-center">Shift in Methodology (Human vs Auto)</h3>
                        <div class="chart-container">
                            <canvas id="evalTrendChart"></canvas>
                        </div>
                    </div>

                    <!-- Chart 2: Focus Areas -->
                    <div class="flex flex-col">
                        <h3 class="text-xl font-semibold text-gray-800 mb-4 text-center">Distribution of Evaluation Targets</h3>
                        <div class="chart-container">
                            <canvas id="focusAreaChart"></canvas>
                        </div>
                    </div>
                </div>
                
                <!-- Chart Details Panel -->
                <div id="chart-details" class="mt-8 p-6 bg-gray-50 rounded-xl border border-gray-200 hidden transition-all duration-300">
                    <h4 id="chart-details-title" class="text-lg font-bold text-gray-800 mb-3"></h4>
                    <div id="chart-details-content" class="flex flex-wrap gap-2"></div>
                </div>
            </div>
        </section>

        <!-- Section 3: Conclusion & Opportunities -->
        <section id="conclusion" class="py-12 max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 mb-12">
            <h2 class="text-3xl font-bold text-gray-900 mb-8">Conclusion & Opportunities</h2>
            
            <div class="grid grid-cols-1 lg:grid-cols-2 gap-8">
                
                <!-- Left Column -->
                <div class="space-y-8">
                    <div class="bg-white p-6 rounded-xl border border-gray-200 shadow-sm">
                        <h3 class="text-xl font-bold text-gray-900 mb-3 flex items-center gap-2">
                            <span class="w-8 h-8 rounded-full bg-blue-100 text-blue-600 flex items-center justify-center text-sm font-bold">1</span>
                            Benchmark Landscape
                        </h3>
                        <p class="text-gray-600 leading-relaxed">
                            Modern T2I benchmarks mostly probe compositionality, reasoning, or a mix of both. 
                            <strong>Composition-focused suites</strong> (e.g., T2I-CompBench++, GenEval) test object presence, attributes, and spatial relations. 
                            <strong>Reasoning-focused ones</strong> (e.g., PhyBench, Commonsense-T2I) target physical laws and abstract reasoning. 
                            <strong>Unified benchmarks</strong> (e.g., T2I-CoReBench, UniGen) combine these into a single semantic evaluation stack.
                        </p>
                    </div>

                    <div class="bg-white p-6 rounded-xl border border-gray-200 shadow-sm">
                        <h3 class="text-xl font-bold text-gray-900 mb-3 flex items-center gap-2">
                            <span class="w-8 h-8 rounded-full bg-teal-100 text-teal-600 flex items-center justify-center text-sm font-bold">2</span>
                            Evaluation Methodology
                        </h3>
                        <p class="text-gray-600 leading-relaxed">
                            The field has largely moved from single scalar metrics (CLIP score, FID) toward <strong>automatic, diagnostic pipelines</strong> that chain open-vocabulary detectors (like UniDet), VQA models (BLIP-style), and MLLMs. These systems explicitly check object presence, attributes, and relations, exposing <em>why</em> a generation fails instead of just giving a global similarity score.
                        </p>
                    </div>
                </div>

                <!-- Right Column -->
                <div class="space-y-8">
                    <!-- Items removed as per request -->
                </div>
            </div>
        </section>
                        <!-- Chart Details Panel -->
                        <div id="chart-details" class="mt-6 p-4 bg-gray-50 rounded-xl border border-gray-200 hidden transition-all">
                            <h4 id="chart-details-title" class="text-sm font-bold text-gray-500 uppercase tracking-wide mb-2">Details</h4>
                            <div id="chart-details-content" class="flex flex-wrap gap-2">
                                <!-- Dynamic Content -->
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

    </main>

    <!-- Modal Overlay -->
    <div id="modal-overlay" class="fixed inset-0 z-50 hidden modal-overlay flex items-center justify-center px-4">
        <div class="bg-white rounded-2xl shadow-2xl max-w-2xl w-full max-h-[90vh] overflow-y-auto flex flex-col transform transition-all scale-95 opacity-0" id="modal-content">
            <!-- Modal Content Injected Here -->
        </div>
    </div>

    <!-- Footer -->
    <footer class="bg-gray-900 text-white py-8 mt-auto">
        <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 flex flex-col md:flex-row justify-between items-center">
            <div class="mb-4 md:mb-0">
                <span class="font-bold text-xl">T2I EvalLens</span>
                <p class="text-gray-400 text-sm mt-1">Literature Review Period: 2022 - 2025</p>
            </div>
            <div class="text-gray-500 text-sm">
                Generated by Canvas Create Webapp
            </div>
        </div>
    </footer>

    <script>
        // --- EXPANDED DATASET ---
        const benchmarks = [
            // 2022
            { 
                id: 1, year: 2022, name: "DrawBench", target: "Challenge Prompts", type: "Human", axes: "Adherence, Fidelity", category: "Other", metrics: ["Human"],
                desc: "The Parti paper benchmark. Small but highly difficult set of prompts testing specific failures.",
                detailedDesc: "Introduced in the Parti paper, DrawBench consists of 200 challenging prompts designed to probe specific failure modes of T2I models. Unlike larger datasets, it focuses on 'stress tests' like counting, complex spatial relations, and rendering text—areas where models historically struggle. Evaluation is strictly human-based comparisons.",
                link: "https://arxiv.org/abs/2205.11487" 
            },
            { 
                id: 2, year: 2022, name: "PartiPrompts", target: "Broad Capabilities", type: "Human", axes: "Composition, World Knowledge", category: "Other", metrics: ["Human"],
                desc: "1600+ prompts across categories like abstract, world knowledge, and art style.",
                detailedDesc: "PartiPrompts is a rich collection of over 1,600 prompts categorized into various domains such as Abstract, World Knowledge, Animals, and Illustrations. It was designed to measure broad capabilities beyond simple object generation. The benchmark relies on human evaluation to judge semantic alignment and image quality.",
                link: "https://arxiv.org/abs/2206.10789" 
            },
            { 
                id: 3, year: 2022, name: "Winoground", target: "Visio-Linguistic", type: "Auto", axes: "Relation, Object Swap", category: "Reasoning", metrics: ["CLIP", "VQA"],
                desc: "Tests if models can distinguish between 'horse riding astronaut' and 'astronaut riding horse'.",
                detailedDesc: "Inspired by the Winograd Schema Challenge, this benchmark tests a model's ability to link language to vision using 'minimal pairs'. It presents two captions with the same words in different orders (e.g., 'the basket is in the mug' vs 'the mug is in the basket') and checks if the model generates distinct, correct images for each.",
                link: "https://arxiv.org/abs/2204.03162" 
            },
            
            // 2023
            { 
                id: 4, year: 2023, name: "T2I-CompBench", target: "Compositionality", type: "Auto (BLIP/CLIP)", axes: "Color, Shape, Texture, Spatial", category: "Composition", metrics: ["BLIP", "CLIP", "VQA"],
                desc: "First comprehensive benchmark specifically for complex compositional reasoning.",
                detailedDesc: "T2I-CompBench addresses the 'bag of words' problem where models ignore attribute binding. It evaluates attribute binding (color, shape, texture), spatial relationships, and complex interactions. It proposes new automated metrics using VQA and specialized captioning models to measure compositional success.",
                link: "https://arxiv.org/abs/2307.06350" 
            },
            { 
                id: 5, year: 2023, name: "TIFA", target: "Fidelity", type: "Auto (VQA)", axes: "Object Existence, Counting", category: "Other", metrics: ["VQA"],
                desc: "Text-to-Image Fidelity Assessment. Converts prompts into QA pairs for VQA models.",
                detailedDesc: "TIFA (Text-to-Image Fidelity Assessment) moves away from CLIP scores by treating evaluation as a Question-Answering task. It automatically generates multiple questions from a prompt (e.g., 'Is there a blue dog?') and uses a VQA model to answer them based on the generated image, providing a granular accuracy score.",
                link: "https://arxiv.org/abs/2303.11897" 
            },
            { 
                id: 6, year: 2023, name: "HEIM", target: "Holistic", type: "Human + Auto", axes: "12 Aspects (Bias, Quality, etc.)", category: "Other", metrics: ["Human", "CLIP", "VQA"],
                desc: "Holistic Evaluation of Text-to-Image Models. Massive scale coverage of non-quality aspects.",
                detailedDesc: "HEIM (Holistic Evaluation of Text-to-Image Models) argues that quality and alignment are insufficient. It evaluates models across 12 distinct aspects including bias, toxicity, aesthetics, originality, and reasoning. It was one of the first to systematically benchmark the 'safety' and 'fairness' dimensions alongside performance.",
                link: "https://arxiv.org/abs/2311.04287" 
            },
            { 
                id: 7, year: 2023, name: "Pick-a-Pic", target: "Human Preference", type: "Human", axes: "Aesthetics, Alignment", category: "Other", metrics: ["Human"],
                desc: "Large-scale crowdsourced dataset of user preferences used to train reward models.",
                detailedDesc: "Pick-a-Pic is a massive dataset of human preferences collected via a web interface where users generate images and pick the best one. This data is crucial for training 'Reward Models' (like ImageReward) that can mimic human aesthetic judgment, facilitating RLHF (Reinforcement Learning from Human Feedback) for images.",
                link: "https://arxiv.org/abs/2305.01749" 
            },
            { 
                id: 8, year: 2023, name: "ImageReward", target: "Reward Modeling", type: "Auto (Model)", axes: "Human Alignment", category: "Other", metrics: ["Reward Model"],
                desc: "A reward model trained to predict human preference, serving as a proxy metric.",
                detailedDesc: "ImageReward is the first general-purpose text-to-image human preference reward model. It outperforms CLIP in aligning with human ranking. It addresses the issue that CLIP prefers images that contain the words, even if the composition is nonsense, whereas ImageReward captures aesthetic and structural quality.",
                link: "https://arxiv.org/abs/2304.05977" 
            },
            { 
                id: 9, year: 2023, name: "GenEval", target: "Object Detection", type: "Auto", axes: "Position, Count, Color", category: "Composition", metrics: ["Object Detection"],
                desc: "Uses standard object detectors to verify if prompt entities actually appear.",
                detailedDesc: "GenEval leverages off-the-shelf object detection models to evaluate T2I fidelity. It focuses on object presence, counting, and spatial positioning. By defining a distinct set of evaluation rules, it provides a 'recall' based metric to see if the model actually generated the noun phrases requested in the prompt.",
                link: "https://arxiv.org/abs/2310.11513" 
            },
            { 
                id: 10, year: 2023, name: "Dall-E 3 Eval", target: "Caption Follow-up", type: "Human", axes: "Long Context, Detail", category: "Other", metrics: ["Human"],
                desc: "Focuses on paragraph-level prompt adherence and detail retention.",
                detailedDesc: "Released with the DALL-E 3 technical report, this evaluation methodology focuses on 'caption upweighting' and long-context adherence. It specifically tests whether the model generates the minute details found in a long, descriptive paragraph, rather than just the main subject.",
                link: "https://cdn.openai.com/papers/dall-e-3.pdf" 
            },

            // 2024
            { 
                id: 11, year: 2024, name: "HRS-Bench", target: "Spatial/Resolution", type: "Auto", axes: "High Resolution, Spatial", category: "Composition", metrics: ["VQA", "Object Detection"],
                desc: "Holistic evaluation for high-res generation and finer spatial details.",
                detailedDesc: "HRS-Bench focuses on High-Resolution and Spatial capability. As models scale to 4K generation, standard metrics fail. This benchmark evaluates how well models maintain structural coherence and spatial relationships (e.g., 'left of', 'inside') at higher pixel counts.",
                link: "https://arxiv.org/abs/2304.05390" 
            },
            { 
                id: 12, year: 2024, name: "RichHF-18K", target: "Rich Human Feedback", type: "Human", axes: "Artifacts, Plausibility", category: "Other", metrics: ["Human"],
                desc: "Annotates specific regions of error (extra fingers, glitch) rather than just a score.",
                detailedDesc: "RichHF moves beyond binary 'good/bad' ratings. It provides fine-grained human feedback including point annotations and heatmaps indicating exactly *where* an image fails (e.g., highlighting a distorted hand or a missing leg). This data allows for training much more precise correction models.",
                link: "https://arxiv.org/abs/2312.10240" 
            },
            { 
                id: 13, year: 2024, name: "SafetyBench-T2I", target: "Safety", type: "Auto", axes: "NSFW, Bias, Copyright", category: "Other", metrics: ["Classification"],
                desc: "Stress-testing safety filters and adversarial robustness.",
                detailedDesc: "SafetyBench is a comprehensive test suite for evaluating the safety of T2I models. It covers categories like NSFW content, political bias, violence, and copyright infringement. It also includes adversarial prompts designed to bypass standard safety filters (jailbreaking).",
                link: "https://arxiv.org/abs/2501.12612" 
            },
            { 
                id: 14, year: 2024, name: "DesignBench", target: "Graphic Design", type: "Auto", axes: "Typography, Layout", category: "Composition", metrics: ["OCR", "VQA"],
                desc: "Evaluating text rendering, logo design, and poster layout capabilities.",
                detailedDesc: "DesignBench evaluates T2I models on graphic design tasks. It specifically tests the ability to render legible text (typography), create cohesive layouts, and generate logo-like structures. It highlights the gap between photorealistic generation and functional graphic design.",
                link: "https://arxiv.org/abs/2310.15144" 
            },
            { 
                id: 15, year: 2024, name: "GeomVerse", target: "Geometry", type: "Auto", axes: "Perspective, 3D Relation", category: "Composition", metrics: ["Geometric"],
                desc: "Testing if 2D images respect 3D geometric transformations and relations.",
                detailedDesc: "GeomVerse evaluates the geometric consistency of generated images. It tests if the model understands 3D concepts like perspective, occlusion, and viewpoint changes. For example, if asked for a 'top-down view', does the model actually flatten the perspective correctly?",
                link: "https://arxiv.org/abs/2312.12241" 
            },
            { 
                id: 16, year: 2024, name: "VQAScore", target: "Alignment", type: "Auto (VQA)", axes: "Complex Alignment", category: "Other", metrics: ["VQA"],
                desc: "Using advanced VQA models to score alignment, outperforming CLIP.",
                detailedDesc: "This paper proposes using state-of-the-art Visual Question Answering models (like CLIP-FlanT5) to score text-image alignment. It demonstrates that VQA models, which are trained on more complex language tasks, correlate far better with human judgment on complex prompts than the standard CLIP score.",
                link: "https://arxiv.org/abs/2404.01291" 
            },
            { 
                id: 17, year: 2024, name: "GenAI-Bench", target: "Compositionality", type: "Auto", axes: "Composition, Reasoning", category: "Composition", metrics: ["VQA", "CLIP"],
                desc: "Evaluating and improving compositional text-to-visual generation.",
                detailedDesc: "GenAI-Bench evaluates the compositional capabilities of T2I models. It focuses on how well models can combine different concepts and attributes to create a coherent image, addressing the challenge of 'binding' attributes to the correct objects.",
                link: "https://arxiv.org/abs/2406.13743" 
            },
            { 
                id: 18, year: 2024, name: "DPG-Bench", target: "Semantic Alignment", type: "Auto", axes: "Dense Prompt, Alignment", category: "Composition", metrics: ["VQA", "LLM-Eval"],
                desc: "Equipping diffusion models with LLMs for enhanced semantic alignment.",
                detailedDesc: "Introduced in the ELLA paper, DPG-Bench focuses on dense prompt generation and semantic alignment. It evaluates how well models can handle complex, detailed prompts by leveraging Large Language Models to enhance the understanding of the diffusion model.",
                link: "https://arxiv.org/abs/2403.05135" 
            },
            { 
                id: 19, year: 2024, name: "ConceptMix", target: "Compositionality", type: "Auto", axes: "Controllable Difficulty", category: "Composition", metrics: ["VQA"],
                desc: "A compositional image generation benchmark with controllable difficulty.",
                detailedDesc: "ConceptMix introduces a benchmark with controllable difficulty for compositional image generation. It allows for a more nuanced evaluation of how models handle increasingly complex combinations of concepts.",
                link: "https://arxiv.org/abs/2408.14339" 
            },
            { 
                id: 20, year: 2024, name: "Commonsense-T2I", target: "Commonsense", type: "Auto", axes: "Commonsense Understanding", category: "Reasoning", metrics: ["VQA", "Human"],
                desc: "Can text-to-image generation models understand commonsense?",
                detailedDesc: "Commonsense-T2I challenges models to generate images that adhere to commonsense knowledge. It tests whether models can avoid generating physically or logically impossible scenarios that violate our understanding of the world.",
                link: "https://arxiv.org/abs/2406.07546" 
            },
            { 
                id: 21, year: 2024, name: "PhyBench", target: "Physics", type: "Auto", axes: "Physical Commonsense", category: "Reasoning", metrics: ["VQA", "Simulation"],
                desc: "A physical commonsense benchmark for evaluating text-to-image models.",
                detailedDesc: "PhyBench evaluates the physical plausibility of generated images. It tests whether models respect laws of physics such as gravity, reflection, and material properties, ensuring that generated scenes are physically consistent.",
                link: "https://arxiv.org/abs/2406.11802" 
            },
            { 
                id: 22, year: 2024, name: "Culture-Bench", target: "Cultural Nuance", type: "Human + Auto", axes: "Global Concepts, Stereotypes", category: "Other", metrics: ["Human", "VQA"],
                desc: "Evaluating generation across diverse cultural contexts and non-Western norms.",
                detailedDesc: "Culture-Bench (CUBE) evaluates the cultural inclusivity of T2I models. It tests prompts related to non-Western festivals, clothing, food, and traditions to check for stereotyping or erasure. It emphasizes the need for models to understand that 'wedding' looks different in India vs. the USA.",
                link: "https://arxiv.org/abs/2407.06863" 
            },

            // 2025
            { 
                id: 23, year: 2025, name: "Logic-T2I", target: "Logical Reasoning", type: "Auto", axes: "Negation, Implication", category: "Reasoning", metrics: ["VQA", "Logic"],
                desc: "Can the model handle 'Not a red ball' or 'If rain, then umbrella'?",
                detailedDesc: "Logic-T2I investigates the logical reasoning capabilities of diffusion models. It focuses on negation ('a room without chairs'), implication ('if it is raining, people hold umbrellas'), and conjunction. It reveals that while models are good at nouns, they struggle significantly with logical operators.",
                link: "https://arxiv.org/abs/2510.00796" 
            },
            { 
                id: 24, year: 2025, name: "Consistency-Story", target: "Consistency", type: "Auto", axes: "Identity, Style Retention", category: "Other", metrics: ["FaceID", "CLIP"],
                desc: "Evaluating subject consistency across multiple prompts for storytelling.",
                detailedDesc: "Focused on narrative generation, this benchmark evaluates 'Identity Consistency'. Can the model generate the exact same character in 10 different poses and settings without morphing their facial features or clothing style? Critical for storyboard and comic creation.",
                link: "https://arxiv.org/abs/2407.08683" 
            },
            { 
                id: 25, year: 2025, name: "TIFF Bench", target: "Fidelity", type: "Auto", axes: "Atomic Faithfulness, VQA", category: "Other", metrics: ["VQA", "Atomic"],
                desc: "Text-Image Fidelity and Faithfulness: Decomposing prompts into atomic claims for verification.",
                detailedDesc: "TIFF Bench (Text-Image Fidelity and Faithfulness) tackles the problem of 'hallucinated' or missing content by decomposing complex text prompts into atomic assertions (e.g., 'There is a cat', 'The cat is black', 'The cat is on a mat'). It then uses specialized VQA or verification models to check each claim against the image, providing a highly granular faithfulness score that correlates better with human judgment than global similarity metrics.",
                link: "https://arxiv.org/abs/2506.02161" 
            },
            { 
                id: 26, year: 2025, name: "FineGrain", target: "Fine-Grained Details", type: "Auto", axes: "Texture, Attributes, Parts", category: "Other", metrics: ["VQA", "Segmentation"],
                desc: "Diagnosing subtle errors in texture, object parts, and attribute leakage.",
                detailedDesc: "FineGrain is a diagnostic benchmark designed to catch the subtle errors that standard metrics miss. Instead of just a pass/fail, it categorizes errors into specific classes such as texture anomalies, missing object sub-parts (e.g., a clock missing hands), and attribute leakage (where the color of one object bleeds into another). It serves as a microscope for model performance.",
                link: "https://finegrainbench.ai/" 
            },
            { 
                id: 27, year: 2025, name: "TIIF-Bench", target: "Instruction Following", type: "Auto", axes: "Instruction Adherence", category: "Composition", metrics: ["VQA", "LLM-Eval"],
                desc: "How does your T2I model follow your instructions?",
                detailedDesc: "TIIF-Bench evaluates how well T2I models follow specific instructions in the prompt. It focuses on the model's ability to adhere to constraints and requirements specified by the user, ensuring that the generated image matches the user's intent.",
                link: "https://arxiv.org/abs/2506.02161" 
            },
            { 
                id: 28, year: 2025, name: "LongBench-T2I", target: "Holistic", type: "Auto", axes: "Complex Instruction", category: "Both", metrics: ["VQA", "Human"],
                desc: "A holistic benchmark and agent framework for complex instruction-based image generation.",
                detailedDesc: "LongBench-T2I evaluates models on complex, long-form instructions. It tests the model's ability to handle detailed and multi-faceted prompts, often requiring an agentic framework to fully resolve the complexity of the generation task.",
                link: "https://arxiv.org/abs/2505.24787" 
            },
            { 
                id: 29, year: 2025, name: "PRISM-Bench", target: "Reasoning", type: "Auto", axes: "Reasoning, Composition", category: "Both", metrics: ["VQA", "Reasoning"],
                desc: "A million-scale text-to-image reasoning dataset and comprehensive benchmark.",
                detailedDesc: "PRISM-Bench is a large-scale benchmark focused on reasoning capabilities. It includes tracks for imagination, entity handling, text rendering, and more, aiming to push the boundaries of what T2I models can 'reason' about during generation.",
                link: "https://arxiv.org/abs/2509.09680" 
            },
            { 
                id: 30, year: 2025, name: "UniGenBench", target: "Semantic Evaluation", type: "Auto", axes: "Unified Semantics", category: "Both", metrics: ["VQA", "Semantic"],
                desc: "A unified semantic evaluation benchmark for text-to-image generation.",
                detailedDesc: "UniGenBench++ proposes a unified framework for semantic evaluation. It aims to standardize how we measure the semantic alignment of generated images with their textual prompts across various dimensions.",
                link: "https://arxiv.org/abs/2510.18701" 
            },
            { 
                id: 31, year: 2025, name: "WISE", target: "Evaluation", type: "Auto", axes: "Evaluation Metric", category: "Other", metrics: ["Metric Eval"],
                desc: "A new benchmark for evaluating T2I models.",
                detailedDesc: "WISE introduces a new methodology for benchmarking T2I models, focusing on robust and scalable evaluation metrics that better reflect human judgment.",
                link: "https://arxiv.org/abs/2503.07265" 
            },
            { 
                id: 32, year: 2025, name: "T2I-ReasonBench", target: "Reasoning", type: "Auto", axes: "Reasoning-Informed", category: "Reasoning", metrics: ["VQA", "Reasoning"],
                desc: "Benchmarking reasoning-informed text-to-image generation.",
                detailedDesc: "T2I-ReasonBench focuses on the intersection of reasoning and generation. It tests whether models can perform implicit reasoning steps required to generate an image that makes sense given the prompt.",
                link: "https://arxiv.org/abs/2508.17472" 
            },
            { 
                id: 33, year: 2025, name: "R2I-Bench", target: "Reasoning", type: "Auto", axes: "Reasoning-Driven", category: "Reasoning", metrics: ["VQA", "Reasoning"],
                desc: "Benchmarking reasoning-driven text-to-image generation.",
                detailedDesc: "R2I-Bench evaluates how well models can handle prompts that require logical deduction or multi-step reasoning to visualize correctly, moving beyond simple object co-occurrence.",
                link: "https://arxiv.org/abs/2505.23493" 
            },
            { 
                id: 34, year: 2025, name: "OneIG-Bench", target: "Nuanced Evaluation", type: "Auto", axes: "Omni-dimensional", category: "Both", metrics: ["VQA", "Human", "Omni"],
                desc: "Omni-dimensional nuanced evaluation for image generation.",
                detailedDesc: "OneIG-Bench proposes a comprehensive, multi-dimensional evaluation framework. It looks at fine-grained details and nuanced aspects of generation that are often overlooked by broader metrics.",
                link: "https://arxiv.org/abs/2506.07977" 
            },
            { 
                id: 35, year: 2025, name: "T2I-CoReBench", target: "Composition & Reasoning", type: "Auto", axes: "Composition, Reasoning", category: "Both", metrics: ["VQA", "Reasoning"],
                desc: "Comprehensive coverage of 12 evaluation dimensions spanning composition and reasoning.",
                detailedDesc: "T2I-CoReBench is a comprehensive benchmark that covers 12 evaluation dimensions, spanning both composition and reasoning scenarios. It distinguishes between high-complexity and simple coverage, providing a detailed map of model capabilities.",
                link: "https://arxiv.org/abs/2509.03516" 
            },
            { 
                id: 36, year: 2025, name: "T2I-CompBench++", target: "Compositionality", type: "Auto", axes: "Attribute Binding, Spatial, Complex", category: "Composition", metrics: ["BLIP", "VQA", "UniDet"],
                desc: "Enhanced benchmark with new metrics for attribute binding, spatial relationships, and complex compositions.",
                detailedDesc: "T2I-CompBench++ extends the original benchmark by introducing new evaluation metrics: Disentangled BLIP-VQA for attribute binding, UniDet-based metric for spatial relationships, and 3-in-1 metric for complex compositions. It provides a more robust evaluation of compositional text-to-image generation.",
                link: "https://arxiv.org/pdf/2307.06350" 
            },
            { 
                id: 37, year: 2025, name: "DiagramEval", target: "Diagram Generation", type: "Auto", axes: "Node Alignment, Path Alignment", category: "Diagram", metrics: ["Graph-based"],
                desc: "Evaluating LLM-Generated Diagrams via Graphs.",
                detailedDesc: "DiagramEval introduces a novel graph-based metric for evaluating LLM-generated SVG diagrams. Unlike pixel-based metrics, it treats diagrams as graphs, assessing 'Node Alignment' (text/shape correctness) and 'Path Alignment' (arrow/relationship correctness). This approach provides fine-grained, discriminative feedback on structural accuracy, addressing the limitations of standard VQA or CLIP-based metrics for schematic content.",
                link: "https://aclanthology.org/2025.emnlp-main.640.pdf" 
            },
            { 
                id: 38, year: 2025, name: "MathemaTikZ", target: "Math Diagrams", type: "Auto", axes: "Spatial, Geometric, Pedagogical", category: "Diagram", metrics: ["VQA", "Code Eval"],
                desc: "Benchmark for K-12 Mathematical Diagram Generation.",
                detailedDesc: "MathemaTikZ is a specialized benchmark for K-12 mathematical diagram generation, comprising 3,793 pairs of natural language descriptions and TikZ code. It rigorously evaluates models on their ability to handle spatial reasoning, geometric constraints, and pedagogical clarity. The study reveals significant gaps in current models' ability to generate accurate, curriculum-aligned visualizations, particularly in maintaining strict geometric relationships.",
                link: "https://spaces-cdn.owlstown.com/blobs/dr1fngasdlei6n26qg8paayz11g5" 
            },
            { 
                id: 39, year: 2023, name: "DiagrammerGPT", target: "Open-Domain Diagrams", type: "Auto", axes: "Layout, Text Rendering", category: "Diagram", metrics: ["Layout", "Text"],
                desc: "Generating Open-Domain, Open-Platform Diagrams via LLM Planning.",
                detailedDesc: "DiagrammerGPT proposes a two-stage 'Planner-Auditor' framework to tackle dense diagram generation. First, an LLM plans the layout and entities; second, a specialized module renders the final image. The paper also introduces 'AI2D-Caption', a densely annotated dataset derived from AI2D, to benchmark the generation of complex, information-rich diagrams that require precise spatial organization and text rendering.",
                link: "https://arxiv.org/abs/2310.12128" 
            },
            { 
                id: 40, year: 2025, name: "From Words to Visual", target: "Structured Visuals", type: "Auto", axes: "Structure, Editing", category: "Diagram", metrics: ["Structure"],
                desc: "A Benchmark and Framework for Text-to-Diagram Generation and Editing.",
                detailedDesc: "This CVPR 2025 paper presents a comprehensive framework for both generating and *editing* structured visuals. It moves beyond static image generation to evaluate how well models can modify existing diagrams based on textual instructions while preserving structural integrity. The benchmark covers a wide range of structured visual types, testing the model's understanding of underlying logical and spatial relationships.",
                link: "https://openaccess.thecvf.com/content/CVPR2025/papers/Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for_CVPR_2025_paper.pdf" 
            },
            { 
                id: 41, year: 2016, name: "AI2D", target: "Diagram Understanding", type: "Human + Auto", axes: "Structure, Semantics", category: "Diagram", metrics: ["VQA", "Parsing"],
                desc: "A Diagram Is Worth A Dozen Images.",
                detailedDesc: "AI2D (A Diagram Is Worth A Dozen Images) is the foundational dataset for diagram understanding. It features over 5,000 diagrams with exhaustive annotations of their constituent parts (blobs, text, arrows) and their relationships. The paper introduces 'Diagram Parse Graphs' (DPG) to model these structures and provides a massive question-answering benchmark (15k+ QA pairs) to test a model's ability to reason about the diagram's semantic content.",
                link: "https://arxiv.org/abs/1603.07396" 
            }
        ];

        // --- INTERACTION LOGIC ---
        const grid = document.getElementById('benchmark-grid');
        const btns = document.querySelectorAll('.filter-btn');
        const modalOverlay = document.getElementById('modal-overlay');
        const modalContent = document.getElementById('modal-content');

        // Modal Logic
        function openModal(id) {
            const b = benchmarks.find(item => item.id === id);
            if (!b) return;

            // Determine badge color
            let badgeColor = "bg-gray-100 text-gray-800";
            if (b.type.includes('Human')) badgeColor = "bg-rose-100 text-rose-800";
            if (b.type.includes('Auto')) badgeColor = "bg-teal-100 text-teal-800";
            if (b.type.includes('+')) badgeColor = "bg-indigo-100 text-indigo-800";

            modalContent.innerHTML = `
                <div class="relative bg-white p-8 rounded-2xl">
                    <button onclick="closeModal()" class="absolute top-4 right-4 text-gray-400 hover:text-gray-600 z-10 bg-white rounded-full p-1 shadow-sm">
                        <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"></path></svg>
                    </button>
                    
                    <div class="mb-6 h-64 overflow-hidden rounded-xl bg-gray-100 relative shadow-sm">
                        <img src="images/${b.id}.png" alt="${b.name} Framework" class="w-full h-full object-contain" onerror="this.parentElement.style.display='none'">
                    </div>

                    <div class="flex items-center gap-3 mb-2">
                        <span class="text-sm font-bold px-3 py-1 rounded-full ${badgeColor}">${b.type}</span>
                        <span class="text-gray-500 font-medium">${b.year}</span>
                    </div>
                    
                    <h2 class="text-3xl font-bold text-gray-900 mb-2">${b.name}</h2>
                    <p class="text-amber-600 font-medium mb-6">${b.target}</p>
                    
                    <div class="prose prose-sm max-w-none text-gray-600 mb-8">
                        <p class="text-lg leading-relaxed text-gray-700">${b.detailedDesc}</p>
                    </div>

                    <div class="bg-gray-50 p-4 rounded-lg border border-gray-100 mb-8">
                        <h4 class="text-xs font-bold text-gray-400 uppercase tracking-wide mb-2">Key Axes Evaluated</h4>
                        <div class="flex flex-wrap gap-2">
                            ${b.axes.split(',').map(ax => `<span class="bg-white border border-gray-200 px-3 py-1 rounded-md text-sm text-gray-700 shadow-sm">${ax.trim()}</span>`).join('')}
                        </div>
                    </div>

                    <a href="${b.link}" target="_blank" rel="noopener noreferrer" class="inline-flex items-center justify-center w-full sm:w-auto px-6 py-3 border border-transparent text-base font-medium rounded-md text-white bg-amber-600 hover:bg-amber-700 transition-colors">
                        View Paper / Resource
                        <svg class="ml-2 -mr-1 w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path></svg>
                    </a>
                </div>
            `;

            modalOverlay.classList.remove('hidden');
            // Small timeout for fade-in effect
            setTimeout(() => {
                modalContent.classList.remove('scale-95', 'opacity-0');
                modalContent.classList.add('scale-100', 'opacity-100');
            }, 10);
            document.body.style.overflow = 'hidden'; // Prevent scrolling
        }

        function closeModal() {
            modalContent.classList.remove('scale-100', 'opacity-100');
            modalContent.classList.add('scale-95', 'opacity-0');
            setTimeout(() => {
                modalOverlay.classList.add('hidden');
                document.body.style.overflow = '';
            }, 300);
        }

        // Close on clicking outside
        modalOverlay.addEventListener('click', (e) => {
            if (e.target === modalOverlay) closeModal();
        });

        // Grid Rendering
        function renderGrid() {
            const searchTerm = document.getElementById('search-input').value.toLowerCase();

            const filtered = benchmarks.filter(b => {
                // Filter Logic
                const matchesSearch = b.name.toLowerCase().includes(searchTerm) || 
                                    b.desc.toLowerCase().includes(searchTerm) ||
                                    b.detailedDesc.toLowerCase().includes(searchTerm);
                
                const matchesYear = activeYear === 'all' || b.year === parseInt(activeYear);
                const matchesType = activeType === 'all' || 
                                  (activeType === 'human' && b.type.toLowerCase().includes('human')) ||
                                  (activeType === 'auto' && b.type.toLowerCase().includes('auto'));

                // Category Filter Logic
                let matchesCategory = true;
                if (activeCategory === 'composition') {
                    matchesCategory = b.category === 'Composition';
                } else if (activeCategory === 'reasoning') {
                    matchesCategory = b.category === 'Reasoning';
                } else if (activeCategory === 'diagram') {
                    matchesCategory = b.category === 'Diagram';
                } else if (activeCategory === 'both') {
                    matchesCategory = b.category === 'Both';
                }

                // Metric Filter Logic
                const matchesMetrics = activeMetrics.length === 0 || activeMetrics.every(m => b.metrics && b.metrics.includes(m));

                return matchesSearch && matchesYear && matchesType && matchesCategory && matchesMetrics;
            });

            // Update Result Count
            const countEl = document.getElementById('result-count');
            countEl.textContent = `Showing ${filtered.length} result${filtered.length !== 1 ? 's' : ''}`;

            // Clear grid
            grid.innerHTML = '';

            if (filtered.length === 0) {
                grid.innerHTML = `
                    <div class="col-span-full text-center py-12">
                        <div class="inline-flex items-center justify-center w-16 h-16 rounded-full bg-gray-100 mb-4">
                            <svg class="w-8 h-8 text-gray-400" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.172 16.172a4 4 0 015.656 0M9 10h.01M15 10h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>
                        </div>
                        <h3 class="text-lg font-medium text-gray-900">No benchmarks found</h3>
                        <p class="text-gray-500 mt-1">Try adjusting your filters or search query.</p>
                    </div>
                `;
                return;
            }

            filtered.forEach(b => {
                const card = document.createElement('div');
                card.className = 'group bg-white rounded-2xl border border-gray-200 p-6 hover:shadow-xl transition-all duration-300 hover:-translate-y-1 cursor-pointer flex flex-col h-full relative overflow-hidden';
                card.onclick = () => openModal(b.id);

                // Badge Logic
                let badgeColor = "bg-gray-100 text-gray-800";
                if (b.type.includes('Human')) badgeColor = "bg-rose-100 text-rose-800";
                if (b.type.includes('Auto')) badgeColor = "bg-teal-100 text-teal-800";
                if (b.type.includes('+')) badgeColor = "bg-indigo-100 text-indigo-800";

                let catBadge = "";
                if (b.category === 'Composition') catBadge = `<span class="text-xs font-bold px-2 py-0.5 rounded-full bg-blue-50 text-blue-700 border border-blue-100">Comp</span>`;
                if (b.category === 'Reasoning') catBadge = `<span class="text-xs font-bold px-2 py-0.5 rounded-full bg-purple-50 text-purple-700 border border-purple-100">Reason</span>`;
                if (b.category === 'Both') catBadge = `<span class="text-xs font-bold px-2 py-0.5 rounded-full bg-fuchsia-50 text-fuchsia-700 border border-fuchsia-100">Both</span>`;

                // Metric Badges (limit to 2 for display)
                let metricBadges = "";
                if (b.metrics && b.metrics.length > 0) {
                    metricBadges = `<div class="flex flex-wrap gap-1 mt-2">
                        ${b.metrics.slice(0, 3).map(m => `<span class="text-[10px] font-medium px-1.5 py-0.5 rounded bg-gray-50 text-gray-500 border border-gray-100">${m}</span>`).join('')}
                        ${b.metrics.length > 3 ? `<span class="text-[10px] font-medium px-1.5 py-0.5 rounded bg-gray-50 text-gray-500 border border-gray-100">+${b.metrics.length - 3}</span>` : ''}
                    </div>`;
                }

                card.innerHTML = `
                    <div class="mb-4 h-40 overflow-hidden rounded-xl bg-gray-50 relative group-hover:bg-white transition-colors">
                         <img src="images/${b.id}.png" alt="${b.name}" class="w-full h-full object-contain mix-blend-multiply opacity-90 group-hover:opacity-100 transition-opacity" onerror="this.parentElement.style.display='none'">
                    </div>

                    <div class="flex items-center justify-between mb-3">
                        <div class="flex items-center gap-2">
                            <span class="text-xs font-bold px-2.5 py-1 rounded-full ${badgeColor}">${b.type.split(' ')[0]}</span>
                            ${catBadge}
                        </div>
                        <span class="text-sm text-gray-400 font-mono">${b.year}</span>
                    </div>
                    
                    <h3 class="text-xl font-bold text-gray-900 mb-2 group-hover:text-amber-600 transition-colors">${b.name}</h3>
                    <p class="text-sm font-medium text-amber-600 mb-3">${b.target}</p>
                    <p class="text-gray-600 text-sm leading-relaxed mb-4 flex-grow">${b.desc}</p>
                    
                    ${metricBadges}

                    <div class="mt-auto pt-4 border-t border-gray-100 flex items-center justify-between text-xs text-gray-500">
                        <span class="truncate max-w-[70%] font-medium" title="${b.axes}">${b.axes}</span>
                        <svg class="w-4 h-4 text-gray-300 group-hover:text-amber-500 transition-colors" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7"></path></svg>
                    </div>
                `;
                grid.appendChild(card);
            });
        }

        // Filter Click Handlers
        let activeYear = 'all';
        let activeType = 'all';
        let activeCategory = 'all';
        let activeMetrics = [];

        btns.forEach(btn => {
            btn.addEventListener('click', () => {
                const filterType = btn.dataset.filter;
                const value = btn.dataset.value;

                // Handle Metric Multi-select
                if (filterType === 'metric') {
                    if (activeMetrics.includes(value)) {
                        activeMetrics = activeMetrics.filter(m => m !== value);
                        btn.classList.remove('bg-amber-100', 'text-amber-800', 'border-amber-200', 'font-bold');
                        btn.classList.add('bg-white', 'text-gray-600', 'border-gray-200', 'hover:bg-gray-50');
                    } else {
                        activeMetrics.push(value);
                        btn.classList.add('bg-amber-100', 'text-amber-800', 'border-amber-200', 'font-bold');
                        btn.classList.remove('bg-white', 'text-gray-600', 'border-gray-200', 'hover:bg-gray-50');
                    }
                    renderGrid();
                    return;
                }

                // Handle Standard Filters
                if (filterType === 'year') activeYear = value;
                if (filterType === 'type') activeType = value;
                if (filterType === 'category') activeCategory = value;

                // Update UI for standard filters
                document.querySelectorAll(`.filter-btn[data-filter="${filterType}"]`).forEach(b => {
                    if (b.dataset.value === value) {
                        b.classList.add('active-filter', 'bg-amber-600', 'text-white', 'shadow-md', 'transform', 'scale-105');
                        b.classList.remove('bg-white', 'text-gray-600', 'hover:bg-gray-50');
                    } else {
                        b.classList.remove('active-filter', 'bg-amber-600', 'text-white', 'shadow-md', 'transform', 'scale-105');
                        b.classList.add('bg-white', 'text-gray-600', 'hover:bg-gray-50');
                    }
                });

                renderGrid();
            });
        });

        // Smooth Scroll
        window.scrollToSection = (id) => {
            document.getElementById(id).scrollIntoView({ behavior: 'smooth' });
        };

        // --- CHARTS ---
        document.addEventListener('DOMContentLoaded', () => {
            renderGrid('all');

            const detailsPanel = document.getElementById('chart-details');
            const detailsTitle = document.getElementById('chart-details-title');
            const detailsContent = document.getElementById('chart-details-content');

            function showDetails(title, items) {
                detailsPanel.classList.remove('hidden');
                detailsTitle.textContent = title;
                detailsContent.innerHTML = items.map(b => 
                    `<a href="${b.link}" target="_blank" class="px-3 py-1 bg-white border border-gray-300 rounded-full text-xs font-medium text-gray-700 hover:bg-amber-50 hover:text-amber-700 hover:border-amber-300 transition-colors flex items-center gap-1">
                        ${b.name}
                        <svg class="w-3 h-3 opacity-50" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path></svg>
                    </a>`
                ).join('');
            }

            // Chart 1: Human vs Auto Trend (Line Chart)
            const ctx1 = document.getElementById('evalTrendChart').getContext('2d');
            new Chart(ctx1, {
                type: 'bar',
                data: {
                    labels: ['2022', '2023', '2024', '2025'],
                    datasets: [
                        {
                            label: 'Human-Centric',
                            data: [2, 2, 1, 0], 
                            backgroundColor: '#e11d48',
                        },
                        {
                            label: 'Automated/Hybrid',
                            data: [1, 5, 11, 14], 
                            backgroundColor: '#0d9488',
                        }
                    ]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    plugins: { legend: { position: 'bottom' } },
                    scales: {
                        y: { beginAtZero: true, ticks: { stepSize: 1 }, grid: { display: false } },
                        x: { grid: { display: false } }
                    },
                    onHover: (e, elements) => {
                        if (elements.length > 0) {
                            const index = elements[0].index;
                            const datasetIndex = elements[0].datasetIndex;
                            const year = ['2022', '2023', '2024', '2025'][index];
                            const type = datasetIndex === 0 ? 'Human' : 'Auto'; // 0 is Human, 1 is Auto
                            
                            const items = benchmarks.filter(b => 
                                b.year.toString() === year && 
                                (type === 'Human' ? b.type.includes('Human') : !b.type.includes('Human'))
                            );
                            
                            showDetails(`${year} - ${type === 'Human' ? 'Human-Centric' : 'Automated/Hybrid'} Benchmarks`, items);
                        }
                    }
                }
            });

            // Chart 2: Focus Areas (Doughnut)
            const focusMap = { 'Comp & Reasoning': [], 'Realism & Alignment': [], 'Safety & Bias': [], 'Text & Instruction': [], 'Other': [] };
            
            benchmarks.forEach(b => {
                const t = b.target.toLowerCase();
                let category = 'Other';
                if (t.includes('comp') || t.includes('spatial') || t.includes('relation') || t.includes('logic') || t.includes('reason') || t.includes('geometry') || t.includes('physics') || t.includes('commonsense') || t.includes('object')) category = 'Comp & Reasoning';
                else if (t.includes('fidelity') || t.includes('preference') || t.includes('align') || t.includes('holistic') || t.includes('faithfulness') || t.includes('consistency') || t.includes('semantic')) category = 'Realism & Alignment';
                else if (t.includes('safety') || t.includes('culture')) category = 'Safety & Bias';
                else if (t.includes('design') || t.includes('challenge') || t.includes('instruction') || t.includes('caption')) category = 'Text & Instruction';
                
                focusMap[category].push(b);
            });

            const ctx2 = document.getElementById('focusAreaChart').getContext('2d');
            new Chart(ctx2, {
                type: 'doughnut',
                data: {
                    labels: Object.keys(focusMap),
                    datasets: [{
                        data: Object.values(focusMap).map(arr => arr.length),
                        backgroundColor: ['#d97706', '#0d9488', '#e11d48', '#4f46e5', '#9ca3af'],
                        borderWidth: 0
                    }]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    plugins: { legend: { position: 'right' } },
                    onHover: (e, elements) => {
                        if (elements.length > 0) {
                            const index = elements[0].index;
                            const category = Object.keys(focusMap)[index];
                            const items = focusMap[category];
                            showDetails(`${category} Benchmarks`, items);
                        }
                    }
                }
            });
        });
    </script>
</body>
</html>