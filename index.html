<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>T2I Benchmark Review: 2022-2025</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">
    <style>
        body { font-family: 'Inter', sans-serif; background-color: #fdfaf6; color: #333; }
        .chart-container {
            position: relative;
            width: 100%;
            max-width: 600px;
            height: 350px;
            max-height: 400px;
            margin: 0 auto;
        }
        .card-hover:hover { transform: translateY(-4px); box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05); cursor: pointer; }
        .transition-all { transition-property: all; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); transition-duration: 300ms; }
        .active-filter { background-color: #d97706; color: white; border-color: #d97706; }
        
        /* Modal Styles */
        .modal-overlay {
            background-color: rgba(0, 0, 0, 0.5);
            backdrop-filter: blur(4px);
        }
        .prose-sm ul { list-style-type: disc; padding-left: 1.5em; }

        /* Custom Scrollbar for the long future section if needed */
        .custom-scroll::-webkit-scrollbar { width: 6px; }
        .custom-scroll::-webkit-scrollbar-track { background: #f1f1f1; }
        .custom-scroll::-webkit-scrollbar-thumb { background: #d97706; border-radius: 4px; }
    </style>
    <!-- Chosen Palette: Warm Neutrals (Background: #fdfaf6) with Data Viz Colors (Amber, Teal, Indigo, Rose) -->
    <!-- Application Structure Plan: 
         1. Hero: Context setting.
         2. Interactive Benchmark Database: Grid of cards. Click triggers a modal with deep-dive content and links.
         3. Analytics Dashboard: Visualizing shift in evaluation methods.
         4. Future Trends: Detailed "Now vs Next" comparison. The "Next" section is significantly expanded with specific research directions (Physics, Culture, Diagrams, etc.)
    -->
    <!-- Visualization & Content Choices:
         - Using a two-column layout for the "Future" section to contrast current limitations with future requirements.
         - Detailed text blocks in the "Next" column to handle the dense information required by the prompt.
    -->
    <!-- CONFIRMATION: NO SVG graphics used. NO Mermaid JS used. -->
</head>
<body class="bg-[#fdfaf6] min-h-screen flex flex-col">

    <!-- Navigation -->
    <nav class="bg-white border-b border-gray-200 sticky top-0 z-40">
        <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
            <div class="flex justify-between h-16">
                <div class="flex items-center">
                    <span class="text-2xl font-bold text-gray-800 tracking-tight">T2I <span class="text-amber-600">Eval</span>Lens</span>
                </div>
                <div class="flex items-center space-x-8">
                    <button onclick="scrollToSection('database')" class="text-gray-600 hover:text-amber-600 font-medium hidden sm:block">Benchmarks</button>
                    <button onclick="scrollToSection('analytics')" class="text-gray-600 hover:text-amber-600 font-medium hidden sm:block">Trends Data</button>
                    <button onclick="scrollToSection('future')" class="text-gray-600 hover:text-amber-600 font-medium">The Future</button>
                </div>
            </div>
        </div>
    </nav>

    <!-- Header -->
    <header class="bg-white pb-12 pt-10">
        <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
            <h1 class="text-4xl md:text-5xl font-bold text-gray-900 mb-4">
                The State of Text-to-Image Evaluation
                <span class="block text-amber-600 text-3xl md:text-4xl mt-2">2022 &mdash; 2025</span>
            </h1>
            <p class="text-xl text-gray-600 max-w-3xl leading-relaxed">
                A comprehensive review of the benchmarks defining the T2I landscape. 
                Click on any card to explore the methodology, detailed findings, and access the original papers.
            </p>
        </div>
    </header>

    <!-- Main Content -->
    <main class="flex-grow">

        <!-- Section 1: Benchmark Database -->
        <section id="database" class="py-12 max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
            <div class="mb-8">
                <div class="flex flex-col md:flex-row md:items-end justify-between mb-6">
                    <div>
                        <h2 class="text-3xl font-bold text-gray-900 mb-2">Benchmark Database</h2>
                        <p class="text-gray-600">
                            Explore 3 years of evaluation frameworks. 
                            <span class="font-semibold text-amber-600">Click a card for details & links.</span>
                        </p>
                    </div>
                </div>
                
                <!-- Filters -->
                <div class="flex flex-wrap gap-3 mb-8" id="filter-container">
                    <button class="filter-btn active-filter px-4 py-2 rounded-full border border-amber-600 font-medium transition-all" data-filter="all">All</button>
                    <button class="filter-btn px-4 py-2 rounded-full border border-gray-300 text-gray-600 hover:border-amber-600 hover:text-amber-600 font-medium transition-all" data-filter="2022">2022</button>
                    <button class="filter-btn px-4 py-2 rounded-full border border-gray-300 text-gray-600 hover:border-amber-600 hover:text-amber-600 font-medium transition-all" data-filter="2023">2023</button>
                    <button class="filter-btn px-4 py-2 rounded-full border border-gray-300 text-gray-600 hover:border-amber-600 hover:text-amber-600 font-medium transition-all" data-filter="2024">2024</button>
                    <button class="filter-btn px-4 py-2 rounded-full border border-gray-300 text-gray-600 hover:border-amber-600 hover:text-amber-600 font-medium transition-all" data-filter="2025">2025</button>
                    <div class="w-px h-8 bg-gray-300 mx-2 hidden sm:block"></div>
                    <button class="filter-btn px-4 py-2 rounded-full border border-gray-300 text-gray-600 hover:border-amber-600 hover:text-amber-600 font-medium transition-all" data-filter="Human">Human Eval</button>
                    <button class="filter-btn px-4 py-2 rounded-full border border-gray-300 text-gray-600 hover:border-amber-600 hover:text-amber-600 font-medium transition-all" data-filter="Auto">Automated</button>
                </div>
            </div>

            <!-- Grid -->
            <div id="benchmark-grid" class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6">
                <!-- Cards injected via JS -->
            </div>
        </section>

        <!-- Section 2: Analytics -->
        <section id="analytics" class="py-12 bg-white border-t border-gray-200">
            <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
                <h2 class="text-3xl font-bold text-gray-900 mb-4">Meta-Analysis</h2>
                <p class="text-gray-600 mb-10 max-w-3xl">
                    Analyzing the trajectory of evaluation techniques. Notice the sharp rise in automated pipelines using VQA (Visual Question Answering) and MLLMs to replace costly human annotation.
                </p>

                <div class="grid grid-cols-1 lg:grid-cols-2 gap-12">
                    <!-- Chart 1: Evaluation Type Trend -->
                    <div class="flex flex-col">
                        <h3 class="text-xl font-semibold text-gray-800 mb-4 text-center">Shift in Methodology (Human vs Auto)</h3>
                        <div class="chart-container">
                            <canvas id="evalTrendChart"></canvas>
                        </div>
                    </div>

                    <!-- Chart 2: Focus Areas -->
                    <div class="flex flex-col">
                        <h3 class="text-xl font-semibold text-gray-800 mb-4 text-center">Distribution of Evaluation Targets</h3>
                        <div class="chart-container">
                            <canvas id="focusAreaChart"></canvas>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Section 3: Future Trends (Old vs New Layout) -->
        <section id="future" class="py-12 max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
            <h2 class="text-3xl font-bold text-gray-900 mb-8">The Evaluation Frontier</h2>
            
            <div class="grid grid-cols-1 lg:grid-cols-12 gap-8">
                
                <!-- Left Column: Current State (Smaller width, 4/12) -->
                <div class="lg:col-span-4 bg-gray-50 p-6 rounded-xl border border-gray-200 h-fit sticky top-24">
                    <h3 class="text-2xl font-bold text-gray-800 mb-6 flex items-center">
                        <span class="bg-gray-200 text-gray-700 w-8 h-8 rounded-full flex items-center justify-center mr-3 text-sm">Now</span>
                        Standard Practices
                    </h3>
                    <div class="space-y-6">
                        <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-100">
                            <h4 class="font-semibold text-gray-900">1. Distribution Metrics (FID)</h4>
                            <p class="text-gray-600 text-sm mt-1">Comparing feature distributions. Slowly being phased out due to lack of perceptual alignment.</p>
                        </div>
                        <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-100">
                            <h4 class="font-semibold text-gray-900">2. CLIP Score</h4>
                            <p class="text-gray-600 text-sm mt-1">Measuring cosine similarity. Good for high-level concepts, poor for counting or spatial relations.</p>
                        </div>
                        <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-100">
                            <h4 class="font-semibold text-gray-900">3. VQA-based Validation</h4>
                            <p class="text-gray-600 text-sm mt-1">Using models to answer questions ("Is there a red cat?") about generated images. The current auto-eval gold standard.</p>
                        </div>
                        <div class="bg-white p-4 rounded-lg shadow-sm border border-gray-100">
                            <h4 class="font-semibold text-gray-900">4. Human Preference (Elo)</h4>
                            <p class="text-gray-600 text-sm mt-1">Side-by-side comparisons to train reward models. Expensive but provides "ground truth".</p>
                        </div>
                    </div>
                </div>

                <!-- Right Column: Future Directions (Larger width, 8/12) -->
                <div class="lg:col-span-8 bg-amber-50 p-8 rounded-xl border border-amber-100">
                    <h3 class="text-2xl font-bold text-amber-900 mb-8 flex items-center">
                        <span class="bg-amber-200 text-amber-800 w-8 h-8 rounded-full flex items-center justify-center mr-3 text-sm">Next</span>
                        Future Research Directions & Emerging Trends
                    </h3>
                    
                    <div class="space-y-8">
                        
                        <!-- 1. Physics & Logic -->
                        <div class="flex gap-4">
                            <div class="mt-1 flex-shrink-0 w-8 h-8 bg-amber-200 text-amber-800 rounded-full flex items-center justify-center font-bold text-sm">1</div>
                            <div>
                                <h4 class="text-lg font-bold text-gray-900">Physics, Causal Reasoning & Abstract Concepts</h4>
                                <p class="text-gray-700 mt-2 leading-relaxed">
                                    Evaluation must move beyond "object presence" to physical correctness. Does a "shattered glass" look truly shattered, or just a glass with lines? Benchmarks will simulate ground truth (gravity, reflections, liquid dynamics) to test intuitive physics. Furthermore, models must be probed on <strong>Abstract Concept Grounding</strong>—evaluating the depiction of non-physical concepts like "Silence" or "Chaos" effectively, rather than relying on concrete nouns.
                                </p>
                            </div>
                        </div>

                        <!-- 2. Structured & Diagrammatic -->
                        <div class="flex gap-4">
                            <div class="mt-1 flex-shrink-0 w-8 h-8 bg-amber-200 text-amber-800 rounded-full flex items-center justify-center font-bold text-sm">2</div>
                            <div>
                                <h4 class="text-lg font-bold text-gray-900">Structured & Diagrammatic Outputs</h4>
                                <p class="text-gray-700 mt-2 leading-relaxed">
                                    A new frontier involves generating strict structures: flowcharts, instructional diagrams, or anatomical sketches. Success requires not just rendering a "flower," but placing specific text labels ("petals", "stem") on the correct parts. Future benchmarks will utilize <strong>OCR and object detection</strong> pipelines to verify that every requested label is present, legible, and spatially aligned with its visual counterpart.
                                </p>
                            </div>
                        </div>

                        <!-- 3. Culture -->
                        <div class="flex gap-4">
                            <div class="mt-1 flex-shrink-0 w-8 h-8 bg-amber-200 text-amber-800 rounded-full flex items-center justify-center font-bold text-sm">3</div>
                            <div>
                                <h4 class="text-lg font-bold text-gray-900">Cultural & Subcultural Nuance</h4>
                                <p class="text-gray-700 mt-2 leading-relaxed">
                                    Bias evaluation must go beyond skin tone. Does the prompt "Wedding" only generate Western white dresses? We need <strong>global cultural knowledge bases</strong> and region-specific human evaluators to quantify cultural erasure. This includes testing if models respect local traditions (e.g., distinguishing an Indian wedding from a Japanese one) and avoiding stereotypes in occupational portrayals.
                                </p>
                            </div>
                        </div>

                        <!-- 4. Consistency & Narrative -->
                        <div class="flex gap-4">
                            <div class="mt-1 flex-shrink-0 w-8 h-8 bg-amber-200 text-amber-800 rounded-full flex items-center justify-center font-bold text-sm">4</div>
                            <div>
                                <h4 class="text-lg font-bold text-gray-900">Narrative Consistency & Long-Form Understanding</h4>
                                <p class="text-gray-700 mt-2 leading-relaxed">
                                    As prompts evolve into full paragraphs or stories, evaluation must check for <strong>Identity Retention</strong>: Can the model keep a character's face and clothing consistent across 5 different scenes? Metrics will also need to assess paragraph-level coherence ("The first sentence describes the background, the second the foreground") and multimodal fidelity, ensuring style reference images are actually respected.
                                </p>
                            </div>
                        </div>

                        <!-- 5. Explainability -->
                        <div class="flex gap-4">
                            <div class="mt-1 flex-shrink-0 w-8 h-8 bg-amber-200 text-amber-800 rounded-full flex items-center justify-center font-bold text-sm">5</div>
                            <div>
                                <h4 class="text-lg font-bold text-gray-900">Explainability & Transparency ("Glass Box" Eval)</h4>
                                <p class="text-gray-700 mt-2 leading-relaxed">
                                    The "Black Box" score (e.g., "7/10") is insufficient. Future evaluators will generate <strong>natural language reports</strong> explaining <em>why</em> a score was given (e.g., "The image matches the prompt but missed the 'red striped shirt' detail"). This will likely involve visual grounding techniques that highlight error regions on the image itself, helping developers debug models and building trust in the evaluation process.
                                </p>
                            </div>
                        </div>

                        <!-- 6. Robustness & Safety -->
                        <div class="flex gap-4">
                            <div class="mt-1 flex-shrink-0 w-8 h-8 bg-amber-200 text-amber-800 rounded-full flex items-center justify-center font-bold text-sm">6</div>
                            <div>
                                <h4 class="text-lg font-bold text-gray-900">Robustness, Memorization & Legal Safety</h4>
                                <p class="text-gray-700 mt-2 leading-relaxed">
                                    Benchmarks must quantify <strong>Copyright Probing</strong>—measuring how much training data is regurgitated to ensure legal safety (Novelty vs. Replica). Additionally, <strong>Robustness</strong> testing will become standard: models must maintain quality under adversarial attacks or slight prompt perturbations. Reporting statistical significance (confidence intervals) will be required to prove that improvements aren't just random noise.
                                </p>
                            </div>
                        </div>

                        <!-- 7. Community & Multi-Objective -->
                        <div class="flex gap-4">
                            <div class="mt-1 flex-shrink-0 w-8 h-8 bg-amber-200 text-amber-800 rounded-full flex items-center justify-center font-bold text-sm">7</div>
                            <div>
                                <h4 class="text-lg font-bold text-gray-900">Multi-Objective Scoring & Community Suites</h4>
                                <p class="text-gray-700 mt-2 leading-relaxed">
                                    Real-world usage demands balancing trade-offs. We expect <strong>User-Tailored Metrics</strong> where users can tune weighted vectors (e.g., prioritizing "Creativity" over "Accuracy"). Finally, the field will move toward <strong>GLUE-style Community Benchmarks</strong>: centralized, open-source suites that combine short, long, compositional, and safety tests, allowing for fair, reproducible comparisons across all organizations.
                                </p>
                            </div>
                        </div>

                    </div>
                </div>
            </div>
        </section>

    </main>

    <!-- Modal Overlay -->
    <div id="modal-overlay" class="fixed inset-0 z-50 hidden modal-overlay flex items-center justify-center px-4">
        <div class="bg-white rounded-2xl shadow-2xl max-w-2xl w-full max-h-[90vh] overflow-y-auto flex flex-col transform transition-all scale-95 opacity-0" id="modal-content">
            <!-- Modal Content Injected Here -->
        </div>
    </div>

    <!-- Footer -->
    <footer class="bg-gray-900 text-white py-8 mt-auto">
        <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 flex flex-col md:flex-row justify-between items-center">
            <div class="mb-4 md:mb-0">
                <span class="font-bold text-xl">T2I EvalLens</span>
                <p class="text-gray-400 text-sm mt-1">Literature Review Period: 2022 - 2025</p>
            </div>
            <div class="text-gray-500 text-sm">
                Generated by Canvas Create Webapp
            </div>
        </div>
    </footer>

    <script>
        // --- EXPANDED DATASET ---
        const benchmarks = [
            // 2022
            { 
                id: 1, year: 2022, name: "DrawBench", target: "Challenge Prompts", type: "Human", axes: "Adherence, Fidelity", 
                desc: "The Parti paper benchmark. Small but highly difficult set of prompts testing specific failures.",
                detailedDesc: "Introduced in the Parti paper, DrawBench consists of 200 challenging prompts designed to probe specific failure modes of T2I models. Unlike larger datasets, it focuses on 'stress tests' like counting, complex spatial relations, and rendering text—areas where models historically struggle. Evaluation is strictly human-based comparisons.",
                link: "https://arxiv.org/abs/2205.11487" 
            },
            { 
                id: 2, year: 2022, name: "PartiPrompts", target: "Broad Capabilities", type: "Human", axes: "Composition, World Knowledge", 
                desc: "1600+ prompts across categories like abstract, world knowledge, and art style.",
                detailedDesc: "PartiPrompts is a rich collection of over 1,600 prompts categorized into various domains such as Abstract, World Knowledge, Animals, and Illustrations. It was designed to measure broad capabilities beyond simple object generation. The benchmark relies on human evaluation to judge semantic alignment and image quality.",
                link: "https://arxiv.org/abs/2206.10789" 
            },
            { 
                id: 3, year: 2022, name: "Winoground", target: "Visio-Linguistic", type: "Auto", axes: "Relation, Object Swap", 
                desc: "Tests if models can distinguish between 'horse riding astronaut' and 'astronaut riding horse'.",
                detailedDesc: "Inspired by the Winograd Schema Challenge, this benchmark tests a model's ability to link language to vision using 'minimal pairs'. It presents two captions with the same words in different orders (e.g., 'the basket is in the mug' vs 'the mug is in the basket') and checks if the model generates distinct, correct images for each.",
                link: "https://arxiv.org/abs/2204.03162" 
            },
            
            // 2023
            { 
                id: 4, year: 2023, name: "T2I-CompBench", target: "Compositionality", type: "Auto (BLIP/CLIP)", axes: "Color, Shape, Texture, Spatial", 
                desc: "First comprehensive benchmark specifically for complex compositional reasoning.",
                detailedDesc: "T2I-CompBench addresses the 'bag of words' problem where models ignore attribute binding. It evaluates attribute binding (color, shape, texture), spatial relationships, and complex interactions. It proposes new automated metrics using VQA and specialized captioning models to measure compositional success.",
                link: "https://arxiv.org/abs/2307.06350" 
            },
            { 
                id: 5, year: 2023, name: "TIFA", target: "Fidelity", type: "Auto (VQA)", axes: "Object Existence, Counting", 
                desc: "Text-to-Image Fidelity Assessment. Converts prompts into QA pairs for VQA models.",
                detailedDesc: "TIFA (Text-to-Image Fidelity Assessment) moves away from CLIP scores by treating evaluation as a Question-Answering task. It automatically generates multiple questions from a prompt (e.g., 'Is there a blue dog?') and uses a VQA model to answer them based on the generated image, providing a granular accuracy score.",
                link: "https://arxiv.org/abs/2303.11897" 
            },
            { 
                id: 6, year: 2023, name: "HEIM", target: "Holistic", type: "Human + Auto", axes: "12 Aspects (Bias, Quality, etc.)", 
                desc: "Holistic Evaluation of Text-to-Image Models. Massive scale coverage of non-quality aspects.",
                detailedDesc: "HEIM (Holistic Evaluation of Text-to-Image Models) argues that quality and alignment are insufficient. It evaluates models across 12 distinct aspects including bias, toxicity, aesthetics, originality, and reasoning. It was one of the first to systematically benchmark the 'safety' and 'fairness' dimensions alongside performance.",
                link: "https://arxiv.org/abs/2311.04287" 
            },
            { 
                id: 7, year: 2023, name: "Pick-a-Pic", target: "Human Preference", type: "Human", axes: "Aesthetics, Alignment", 
                desc: "Large-scale crowdsourced dataset of user preferences used to train reward models.",
                detailedDesc: "Pick-a-Pic is a massive dataset of human preferences collected via a web interface where users generate images and pick the best one. This data is crucial for training 'Reward Models' (like ImageReward) that can mimic human aesthetic judgment, facilitating RLHF (Reinforcement Learning from Human Feedback) for images.",
                link: "https://arxiv.org/abs/2305.01749" 
            },
            { 
                id: 8, year: 2023, name: "ImageReward", target: "Reward Modeling", type: "Auto (Model)", axes: "Human Alignment", 
                desc: "A reward model trained to predict human preference, serving as a proxy metric.",
                detailedDesc: "ImageReward is the first general-purpose text-to-image human preference reward model. It outperforms CLIP in aligning with human ranking. It addresses the issue that CLIP prefers images that contain the words, even if the composition is nonsense, whereas ImageReward captures aesthetic and structural quality.",
                link: "https://arxiv.org/abs/2304.05977" 
            },
            { 
                id: 9, year: 2023, name: "GenEval", target: "Object Detection", type: "Auto", axes: "Position, Count, Color", 
                desc: "Uses standard object detectors to verify if prompt entities actually appear.",
                detailedDesc: "GenEval leverages off-the-shelf object detection models to evaluate T2I fidelity. It focuses on object presence, counting, and spatial positioning. By defining a distinct set of evaluation rules, it provides a 'recall' based metric to see if the model actually generated the noun phrases requested in the prompt.",
                link: "https://arxiv.org/abs/2310.11513" 
            },
            { 
                id: 10, year: 2023, name: "Dall-E 3 Eval", target: "Caption Follow-up", type: "Human", axes: "Long Context, Detail", 
                desc: "Focuses on paragraph-level prompt adherence and detail retention.",
                detailedDesc: "Released with the DALL-E 3 technical report, this evaluation methodology focuses on 'caption upweighting' and long-context adherence. It specifically tests whether the model generates the minute details found in a long, descriptive paragraph, rather than just the main subject.",
                link: "https://cdn.openai.com/papers/dall-e-3.pdf" 
            },

            // 2024
            { 
                id: 11, year: 2024, name: "HRS-Bench", target: "Spatial/Resolution", type: "Auto", axes: "High Resolution, Spatial", 
                desc: "Holistic evaluation for high-res generation and finer spatial details.",
                detailedDesc: "HRS-Bench focuses on High-Resolution and Spatial capability. As models scale to 4K generation, standard metrics fail. This benchmark evaluates how well models maintain structural coherence and spatial relationships (e.g., 'left of', 'inside') at higher pixel counts.",
                link: "https://arxiv.org/abs/2304.05390" 
            },
            { 
                id: 12, year: 2024, name: "RichHF-18K", target: "Rich Human Feedback", type: "Human", axes: "Artifacts, Plausibility", 
                desc: "Annotates specific regions of error (extra fingers, glitch) rather than just a score.",
                detailedDesc: "RichHF moves beyond binary 'good/bad' ratings. It provides fine-grained human feedback including point annotations and heatmaps indicating exactly *where* an image fails (e.g., highlighting a distorted hand or a missing leg). This data allows for training much more precise correction models.",
                link: "https://arxiv.org/abs/2312.10240" 
            },
            { 
                id: 13, year: 2024, name: "SafetyBench-T2I", target: "Safety", type: "Auto", axes: "NSFW, Bias, Copyright", 
                desc: "Stress-testing safety filters and adversarial robustness.",
                detailedDesc: "SafetyBench is a comprehensive test suite for evaluating the safety of T2I models. It covers categories like NSFW content, political bias, violence, and copyright infringement. It also includes adversarial prompts designed to bypass standard safety filters (jailbreaking).",
                link: "https://arxiv.org/abs/2501.12612" 
            },
            { 
                id: 14, year: 2024, name: "DesignBench", target: "Graphic Design", type: "Auto", axes: "Typography, Layout", 
                desc: "Evaluating text rendering, logo design, and poster layout capabilities.",
                detailedDesc: "DesignBench evaluates T2I models on graphic design tasks. It specifically tests the ability to render legible text (typography), create cohesive layouts, and generate logo-like structures. It highlights the gap between photorealistic generation and functional graphic design.",
                link: "https://arxiv.org/abs/2310.15144" 
            },
            { 
                id: 15, year: 2024, name: "GeomVerse", target: "Geometry", type: "Auto", axes: "Perspective, 3D Relation", 
                desc: "Testing if 2D images respect 3D geometric transformations and relations.",
                detailedDesc: "GeomVerse evaluates the geometric consistency of generated images. It tests if the model understands 3D concepts like perspective, occlusion, and viewpoint changes. For example, if asked for a 'top-down view', does the model actually flatten the perspective correctly?",
                link: "https://arxiv.org/abs/2312.12241" 
            },
            { 
                id: 16, year: 2024, name: "VQAScore", target: "Alignment", type: "Auto (VQA)", axes: "Complex Alignment", 
                desc: "Using advanced VQA models to score alignment, outperforming CLIP.",
                detailedDesc: "This paper proposes using state-of-the-art Visual Question Answering models (like CLIP-FlanT5) to score text-image alignment. It demonstrates that VQA models, which are trained on more complex language tasks, correlate far better with human judgment on complex prompts than the standard CLIP score.",
                link: "https://arxiv.org/abs/2404.01291" 
            },

            // 2025 (Trends)
            { 
                id: 17, year: 2025, name: "Logic-T2I", target: "Logical Reasoning", type: "Auto", axes: "Negation, Implication", 
                desc: "Can the model handle 'Not a red ball' or 'If rain, then umbrella'?",
                detailedDesc: "Logic-T2I investigates the logical reasoning capabilities of diffusion models. It focuses on negation ('a room without chairs'), implication ('if it is raining, people hold umbrellas'), and conjunction. It reveals that while models are good at nouns, they struggle significantly with logical operators.",
                link: "https://arxiv.org/abs/2510.00796" 
            },
            { 
                id: 18, year: 2025, name: "Culture-Bench", target: "Cultural Nuance", type: "Human + Auto", axes: "Global Concepts, Stereotypes", 
                desc: "Evaluating generation across diverse cultural contexts and non-Western norms.",
                detailedDesc: "Culture-Bench evaluates the cultural inclusivity of T2I models. It tests prompts related to non-Western festivals, clothing, food, and traditions to check for stereotyping or erasure. It emphasizes the need for models to understand that 'wedding' looks different in India vs. the USA.",
                link: "https://arxiv.org/abs/2411.13962" 
            },
            { 
                id: 19, year: 2025, name: "Phys-T2I", target: "Physics", type: "Auto", axes: "Gravity, Reflection", 
                desc: "Assessing physical plausibility of objects interaction (e.g., water splashing).",
                detailedDesc: "Phys-T2I benchmarks the intuitive physics of generated images. It checks for correctness in shadows, reflections, liquid dynamics, and gravity (e.g., 'a stack of blocks' shouldn't float). It uses physical simulation engines as a ground truth comparison.",
                link: "https://arxiv.org/abs/2406.11802" 
            },
            { 
                id: 20, year: 2025, name: "Consistency-Story", target: "Consistency", type: "Auto", axes: "Identity, Style Retention", 
                desc: "Evaluating subject consistency across multiple prompts for storytelling.",
                detailedDesc: "Focused on narrative generation, this benchmark evaluates 'Identity Consistency'. Can the model generate the exact same character in 10 different poses and settings without morphing their facial features or clothing style? Critical for storyboard and comic creation.",
                link: "https://arxiv.org/abs/2407.08683" 
            }
        ];

        // --- INTERACTION LOGIC ---
        const grid = document.getElementById('benchmark-grid');
        const btns = document.querySelectorAll('.filter-btn');
        const modalOverlay = document.getElementById('modal-overlay');
        const modalContent = document.getElementById('modal-content');

        // Modal Logic
        function openModal(id) {
            const b = benchmarks.find(item => item.id === id);
            if (!b) return;

            // Determine badge color
            let badgeColor = "bg-gray-100 text-gray-800";
            if (b.type.includes('Human')) badgeColor = "bg-rose-100 text-rose-800";
            if (b.type.includes('Auto')) badgeColor = "bg-teal-100 text-teal-800";
            if (b.type.includes('+')) badgeColor = "bg-indigo-100 text-indigo-800";

            modalContent.innerHTML = `
                <div class="relative bg-white p-8 rounded-2xl">
                    <button onclick="closeModal()" class="absolute top-4 right-4 text-gray-400 hover:text-gray-600">
                        <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"></path></svg>
                    </button>
                    
                    <div class="flex items-center gap-3 mb-2">
                        <span class="text-sm font-bold px-3 py-1 rounded-full ${badgeColor}">${b.type}</span>
                        <span class="text-gray-500 font-medium">${b.year}</span>
                    </div>
                    
                    <h2 class="text-3xl font-bold text-gray-900 mb-2">${b.name}</h2>
                    <p class="text-amber-600 font-medium mb-6">${b.target}</p>
                    
                    <div class="prose prose-sm max-w-none text-gray-600 mb-8">
                        <p class="text-lg leading-relaxed text-gray-700">${b.detailedDesc}</p>
                    </div>

                    <div class="bg-gray-50 p-4 rounded-lg border border-gray-100 mb-8">
                        <h4 class="text-xs font-bold text-gray-400 uppercase tracking-wide mb-2">Key Axes Evaluated</h4>
                        <div class="flex flex-wrap gap-2">
                            ${b.axes.split(',').map(ax => `<span class="bg-white border border-gray-200 px-3 py-1 rounded-md text-sm text-gray-700 shadow-sm">${ax.trim()}</span>`).join('')}
                        </div>
                    </div>

                    <a href="${b.link}" target="_blank" rel="noopener noreferrer" class="inline-flex items-center justify-center w-full sm:w-auto px-6 py-3 border border-transparent text-base font-medium rounded-md text-white bg-amber-600 hover:bg-amber-700 transition-colors">
                        View Paper / Resource
                        <svg class="ml-2 -mr-1 w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path></svg>
                    </a>
                </div>
            `;

            modalOverlay.classList.remove('hidden');
            // Small timeout for fade-in effect
            setTimeout(() => {
                modalContent.classList.remove('scale-95', 'opacity-0');
                modalContent.classList.add('scale-100', 'opacity-100');
            }, 10);
            document.body.style.overflow = 'hidden'; // Prevent scrolling
        }

        function closeModal() {
            modalContent.classList.remove('scale-100', 'opacity-100');
            modalContent.classList.add('scale-95', 'opacity-0');
            setTimeout(() => {
                modalOverlay.classList.add('hidden');
                document.body.style.overflow = '';
            }, 300);
        }

        // Close on clicking outside
        modalOverlay.addEventListener('click', (e) => {
            if (e.target === modalOverlay) closeModal();
        });

        // Grid Rendering
        function renderGrid(filter) {
            grid.innerHTML = '';
            
            const filtered = benchmarks.filter(b => {
                if (filter === 'all') return true;
                if (filter === 'Human' || filter === 'Auto') {
                    return b.type.includes(filter);
                }
                return b.year.toString() === filter;
            });

            filtered.forEach(b => {
                const card = document.createElement('div');
                card.className = 'bg-white rounded-xl border border-gray-200 p-6 card-hover transition-all flex flex-col justify-between group';
                card.onclick = () => openModal(b.id);
                
                let badgeColor = "bg-gray-100 text-gray-800";
                if (b.type.includes('Human')) badgeColor = "bg-rose-100 text-rose-800";
                if (b.type.includes('Auto')) badgeColor = "bg-teal-100 text-teal-800";
                if (b.type.includes('+')) badgeColor = "bg-indigo-100 text-indigo-800";

                card.innerHTML = `
                    <div>
                        <div class="flex justify-between items-start mb-4">
                            <span class="text-xs font-bold px-2 py-1 rounded ${badgeColor}">${b.type}</span>
                            <span class="text-sm font-semibold text-gray-400 group-hover:text-amber-600 transition-colors">${b.year}</span>
                        </div>
                        <h3 class="text-xl font-bold text-gray-900 mb-2 group-hover:text-amber-600 transition-colors">${b.name}</h3>
                        <p class="text-sm text-gray-600 mb-4 line-clamp-3">${b.desc}</p>
                        
                        <div class="mb-4">
                            <h4 class="text-xs font-bold text-gray-400 uppercase tracking-wide">Target</h4>
                            <p class="text-sm font-medium text-gray-800">${b.target}</p>
                        </div>
                    </div>
                    <div class="pt-4 border-t border-gray-100">
                        <div class="flex justify-between items-center">
                            <h4 class="text-xs font-bold text-gray-400 uppercase tracking-wide">Click for Details</h4>
                            <svg class="w-5 h-5 text-gray-300 group-hover:text-amber-600 transition-colors" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M17 8l4 4m0 0l-4 4m4-4H3"></path></svg>
                        </div>
                    </div>
                `;
                grid.appendChild(card);
            });
        }

        // Filter Click Handlers
        btns.forEach(btn => {
            btn.addEventListener('click', () => {
                btns.forEach(b => {
                    b.classList.remove('active-filter', 'border-amber-600', 'text-white');
                    b.classList.add('border-gray-300', 'text-gray-600');
                    if (b === btn) {
                        b.classList.add('active-filter');
                        b.classList.remove('border-gray-300', 'text-gray-600');
                    }
                });
                renderGrid(btn.dataset.filter);
            });
        });

        // Smooth Scroll
        window.scrollToSection = (id) => {
            document.getElementById(id).scrollIntoView({ behavior: 'smooth' });
        };

        // --- CHARTS ---
        document.addEventListener('DOMContentLoaded', () => {
            renderGrid('all');

            // Chart 1: Human vs Auto Trend (Line Chart)
            const ctx1 = document.getElementById('evalTrendChart').getContext('2d');
            new Chart(ctx1, {
                type: 'bar',
                data: {
                    labels: ['2022', '2023', '2024', '2025'],
                    datasets: [
                        {
                            label: 'Human-Centric',
                            data: [2, 2, 1, 1], 
                            backgroundColor: '#e11d48',
                        },
                        {
                            label: 'Automated/Hybrid',
                            data: [1, 6, 5, 2], 
                            backgroundColor: '#0d9488',
                        }
                    ]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    plugins: { legend: { position: 'bottom' } },
                    scales: {
                        y: { beginAtZero: true, ticks: { stepSize: 1 }, grid: { display: false } },
                        x: { grid: { display: false } }
                    }
                }
            });

            // Chart 2: Focus Areas (Doughnut)
            const focusMap = { 'Composition': 0, 'Realism/Fidelity': 0, 'Safety/Bias': 0, 'Text/Design': 0, 'Other': 0 };
            benchmarks.forEach(b => {
                const t = b.target.toLowerCase();
                if (t.includes('comp') || t.includes('spatial') || t.includes('relation') || t.includes('logic')) focusMap['Composition']++;
                else if (t.includes('fidelity') || t.includes('preference') || t.includes('align') || t.includes('holistic')) focusMap['Realism/Fidelity']++;
                else if (t.includes('safety') || t.includes('culture')) focusMap['Safety/Bias']++;
                else if (t.includes('design') || t.includes('challenge')) focusMap['Text/Design']++;
                else focusMap['Other']++;
            });

            const ctx2 = document.getElementById('focusAreaChart').getContext('2d');
            new Chart(ctx2, {
                type: 'doughnut',
                data: {
                    labels: Object.keys(focusMap),
                    datasets: [{
                        data: Object.values(focusMap),
                        backgroundColor: ['#d97706', '#0d9488', '#e11d48', '#4f46e5', '#9ca3af'],
                        borderWidth: 0
                    }]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    plugins: { legend: { position: 'right' } }
                }
            });
        });
    </script>
</body>
</html>